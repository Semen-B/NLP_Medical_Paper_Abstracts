{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78c500d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce RTX 3060 Laptop GPU (UUID: GPU-63841278-b94a-af1d-b385-7d84275529fb)\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU\n",
    "!nvidia-smi -L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b45d932",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\n",
      "Your branch is up to date with 'origin/master'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\tmodified:   09_SkimLit_nlp_milestone_project_2.ipynb\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t.ipynb_checkpoints/\n",
      "\t02. Neural Network Classification with TensorFlow.ipynb\n",
      "\t03. Convolutional Neural Networks and Computer Vision with TensorFlow.ipynb\n",
      "\t04_transfer_learning_in_tensorflow_part_1_feature_extraction.ipynb\n",
      "\t05_transfer_learning_in_tensorflow_part_2_fine_tuning.ipynb\n",
      "\t\"06. Transfer Learning with TensorFlow Part 3 Scaling up (\\360\\237\\215\\224\\360\\237\\221\\201 Food Vision mini).ipynb\"\n",
      "\t06_101_food_class_10_percent_saved_big_dog_model.zip\n",
      "\t06_101_food_class_10_percent_saved_big_dog_model.zip.1\n",
      "\t06_101_food_class_10_percent_saved_big_dog_model/\n",
      "\t07_efficientnetb0_feature_extract_model_mixed_precision.zip\n",
      "\t07_efficientnetb0_feature_extract_model_mixed_precision/\n",
      "\t07_food_vision_milestone_project_1.ipynb\n",
      "\t08_introduction_to_nlp_in_tensorflow.ipynb\n",
      "\t101_classes_10_percent_data_model_checkpoint.data-00000-of-00001\n",
      "\t101_classes_10_percent_data_model_checkpoint.index\n",
      "\t101_food_classes_10_percent.zip\n",
      "\t101_food_classes_10_percent/\n",
      "\t10_food_classes_10_percent.zip\n",
      "\t10_food_classes_10_percent.zip.1\n",
      "\t10_food_classes_10_percent/\n",
      "\t10_food_classes_1_percent.zip\n",
      "\t10_food_classes_1_percent.zip.1\n",
      "\t10_food_classes_1_percent/\n",
      "\t10_food_classes_all_data/\n",
      "\tBig_dog_model/\n",
      "\tNLP-Neural-Networks-for-Joint-Sentence-Classification-in-Medical-Paper-Abstracts/\n",
      "\tTransfer_lerning_myself_test.ipynb\n",
      "\tUntitled.ipynb\n",
      "\tUntitled1.ipynb\n",
      "\tUntitled2.ipynb\n",
      "\t__MACOSX/\n",
      "\t__pycache__/\n",
      "\tcheckpoint\n",
      "\tconfusion_matrix.png\n",
      "\tdesktop.ini\n",
      "\tfood_vision_model_checkpoints.data-00000-of-00001\n",
      "\tfood_vision_model_checkpoints.index\n",
      "\thelper_functions.py\n",
      "\thelper_functions.py.1\n",
      "\thelper_functions.py.2\n",
      "\thelper_functions.py.3\n",
      "\thelper_functions.py.4\n",
      "\tmodel_checkpoints_food_vision/\n",
      "\tmodel_checkpoints_food_vision_2/\n",
      "\tmy_LSTM/\n",
      "\tmy_LSTM_3/\n",
      "\tmy_LSTM_5/\n",
      "\tmy_LSTM_7/\n",
      "\tmy_LSTM_8/\n",
      "\tmy_dir_for_twiter_1/\n",
      "\tnlp_getting_started.zip\n",
      "\tpizza_steak/\n",
      "\tpubmed-rct/\n",
      "\tsample_submission.csv\n",
      "\tsaved_trained_model_multyclass/\n",
      "\tten_percent_model_checkpoints_weights/\n",
      "\ttensorflow_hub/\n",
      "\ttesla_model_3/\n",
      "\ttest.csv\n",
      "\ttrain.csv\n",
      "\ttraining_logs/\n",
      "\ttransfer_learning/\n",
      "\ttransfer_learning_7/\n",
      "\ttransfet_lerning/\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd8a6fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'NLP-Neural-Networks-for-Joint-Sentence-Classification-in-Medical-Paper-Abstracts' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/Semen-B/NLP-Neural-Networks-for-Joint-Sentence-Classification-in-Medical-Paper-Abstracts.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79fa9ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: LF will be replaced by CRLF in 09_SkimLit_nlp_milestone_project_2.ipynb.\n",
      "The file will have its original line endings in your working directory\n"
     ]
    }
   ],
   "source": [
    "! git add 09_SkimLit_nlp_milestone_project_2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bec39327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 8a1c493] sixth commit\n",
      " 1 file changed, 306 insertions(+), 1087 deletions(-)\n"
     ]
    }
   ],
   "source": [
    "! git commit -m \"sixth commit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "465635d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error: remote origin already exists.\n"
     ]
    }
   ],
   "source": [
    "! git remote add origin https://github.com/Semen-B/NLP_Medical_Paper_Abstracts.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2638cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git branch -M master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abb19766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* master\n",
      "  tmp\n"
     ]
    }
   ],
   "source": [
    "! git branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "029dd88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch 'master' set up to track remote branch 'master' from 'origin'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/Semen-B/NLP_Medical_Paper_Abstracts.git\n",
      "   d500778..8a1c493  master -> master\n"
     ]
    }
   ],
   "source": [
    "! git push -u origin master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07bf5105",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f888a0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'pubmed-rct' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ff290fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is AA66-8B4D\n",
      "\n",
      " Directory of C:\\Users\\semen\\Desktop\\pythonProject\\pubmed-rct\n",
      "\n",
      "01.05.2022  17:05    <DIR>          .\n",
      "01.05.2022  17:05    <DIR>          ..\n",
      "01.05.2022  17:14    <DIR>          PubMed_200k_RCT\n",
      "01.05.2022  17:16    <DIR>          PubMed_200k_RCT_numbers_replaced_with_at_sign\n",
      "01.05.2022  17:05    <DIR>          PubMed_20k_RCT\n",
      "01.05.2022  17:05    <DIR>          PubMed_20k_RCT_numbers_replaced_with_at_sign\n",
      "01.05.2022  17:05             2ÿ403 README.md\n",
      "               1 File(s)          2ÿ403 bytes\n",
      "               6 Dir(s)  311ÿ839ÿ993ÿ856 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir pubmed-rct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7f80738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is AA66-8B4D\n",
      "\n",
      " Directory of C:\\Users\\semen\\Desktop\\pythonProject\\pubmed-rct\\PubMed_20k_RCT_numbers_replaced_with_at_sign\n",
      "\n",
      "01.05.2022  17:05    <DIR>          .\n",
      "01.05.2022  17:05    <DIR>          ..\n",
      "01.05.2022  17:05         4ÿ880ÿ409 dev.txt\n",
      "01.05.2022  17:05         4ÿ846ÿ504 test.txt\n",
      "01.05.2022  17:05        29ÿ118ÿ832 train.txt\n",
      "               3 File(s)     38ÿ845ÿ745 bytes\n",
      "               2 Dir(s)  311ÿ839ÿ870ÿ976 bytes free\n"
     ]
    }
   ],
   "source": [
    "# Check what files are in the PubMed_20K dataset \n",
    "!dir pubmed-rct\\PubMed_20k_RCT_numbers_replaced_with_at_sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36290cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to read the lines of a document\n",
    "def get_lines(filename):\n",
    "  \"\"\"\n",
    "  Reads filename (a text file) and returns the lines of text as a list.\n",
    "  \n",
    "  Args:\n",
    "      filename: a string containing the target filepath to read.\n",
    "  \n",
    "  Returns:\n",
    "      A list of strings with one string per line from the target filename.\n",
    "      For example:\n",
    "      [\"this is the first line of filename\",\n",
    "       \"this is the second line of filename\",\n",
    "       \"...\"]\n",
    "  \"\"\"\n",
    "  with open(filename, \"r\") as f:\n",
    "    return f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a419acd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_list_of_dic(filename):\n",
    "    my_list_dic=[]\n",
    "    my_list = str(get_lines(filename))\n",
    "    sample = my_list.split('###')\n",
    "    for f in range(len(sample)):\n",
    "        count = 0\n",
    "        separator = \"\\\\n\"\n",
    "        semple_1 = sample[f].split(separator)\n",
    "        total_lines = len(semple_1)-4\n",
    "        for i in semple_1[1:-2]:\n",
    "            temp_dic = {}\n",
    "            _key, value, *other = i.split('\\\\t')\n",
    "            temp_dic['target'] = ''.join(char for char in _key if char.isalnum())\n",
    "            temp_dic['text'] = value.lower()\n",
    "            temp_dic['line_number'] = count\n",
    "            temp_dic['total_lines'] = total_lines\n",
    "            my_list_dic.append(temp_dic)\n",
    "            count+=1\n",
    "    return my_list_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2dbf86e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'target': 'OBJECTIVE',\n",
       "  'text': 'to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n",
       "  'line_number': 0,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n",
       "  'line_number': 1,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .',\n",
       "  'line_number': 2,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'pain was assessed using the visual analog pain scale ( @-@ mm ) .',\n",
       "  'line_number': 3,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .',\n",
       "  'line_number': 4,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'serum levels of interleukin @ ( il-@ ) , il-@ , tumor necrosis factor ( tnf ) - , and high-sensitivity c-reactive protein ( hscrp ) were measured .',\n",
       "  'line_number': 5,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'there was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , pga , and @mwd at @ weeks .',\n",
       "  'line_number': 6,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'the mean difference between treatment arms ( @ % ci ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .',\n",
       "  'line_number': 7,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'further , there was a clinically relevant reduction in the serum levels of il-@ , il-@ , tnf - , and hscrp at @ weeks in the intervention group when compared to the placebo group .',\n",
       "  'line_number': 8,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'these differences remained significant at @ weeks .',\n",
       "  'line_number': 9,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'the outcome measures in rheumatology clinical trials-osteoarthritis research society international responder rate was @ % in the intervention group and @ % in the placebo group ( p < @ ) .',\n",
       "  'line_number': 10,\n",
       "  'total_lines': 11},\n",
       " {'target': 'CONCLUSIONS',\n",
       "  'text': 'low-dose oral prednisolone had both a short-term and a longer sustained effect resulting in less knee pain , better physical function , and attenuation of systemic inflammation in older patients with knee oa ( clinicaltrials.gov identifier nct@ ) .',\n",
       "  'line_number': 11,\n",
       "  'total_lines': 11},\n",
       " {'target': 'BACKGROUND',\n",
       "  'text': 'emotional eating is associated with overeating and the development of obesity .',\n",
       "  'line_number': 0,\n",
       "  'total_lines': 10},\n",
       " {'target': 'BACKGROUND',\n",
       "  'text': 'yet , empirical evidence for individual ( trait ) differences in emotional eating and cognitive mechanisms that contribute to eating during sad mood remain equivocal .',\n",
       "  'line_number': 1,\n",
       "  'total_lines': 10},\n",
       " {'target': 'OBJECTIVE',\n",
       "  'text': 'the aim of this study was to test if attention bias for food moderates the effect of self-reported emotional eating during sad mood ( vs neutral mood ) on actual food intake .',\n",
       "  'line_number': 2,\n",
       "  'total_lines': 10},\n",
       " {'target': 'OBJECTIVE',\n",
       "  'text': 'it was expected that emotional eating is predictive of elevated attention for food and higher food intake after an experimentally induced sad mood and that attentional maintenance on food predicts food intake during a sad versus a neutral mood .',\n",
       "  'line_number': 3,\n",
       "  'total_lines': 10},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'participants ( n = @ ) were randomly assigned to one of the two experimental mood induction conditions ( sad/neutral ) .',\n",
       "  'line_number': 4,\n",
       "  'total_lines': 10},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'attentional biases for high caloric foods were measured by eye tracking during a visual probe task with pictorial food and neutral stimuli .',\n",
       "  'line_number': 5,\n",
       "  'total_lines': 10},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'self-reported emotional eating was assessed with the dutch eating behavior questionnaire ( debq ) and ad libitum food intake was tested by a disguised food offer .',\n",
       "  'line_number': 6,\n",
       "  'total_lines': 10},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'hierarchical multivariate regression modeling showed that self-reported emotional eating did not account for changes in attention allocation for food or food intake in either condition .',\n",
       "  'line_number': 7,\n",
       "  'total_lines': 10},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'yet , attention maintenance on food cues was significantly related to increased intake specifically in the neutral condition , but not in the sad mood condition .',\n",
       "  'line_number': 8,\n",
       "  'total_lines': 10},\n",
       " {'target': 'CONCLUSIONS',\n",
       "  'text': 'the current findings show that self-reported emotional eating ( based on the debq ) might not validly predict who overeats when sad , at least not in a laboratory setting with healthy women .',\n",
       "  'line_number': 9,\n",
       "  'total_lines': 10},\n",
       " {'target': 'CONCLUSIONS',\n",
       "  'text': 'results further suggest that attention maintenance on food relates to eating motivation when in a neutral affective state , and might therefore be a cognitive mechanism contributing to increased food intake in general , but maybe not during sad mood .',\n",
       "  'line_number': 10,\n",
       "  'total_lines': 10},\n",
       " {'target': 'BACKGROUND',\n",
       "  'text': 'although working smoke alarms halve deaths in residential fires , many households do not keep alarms operational .',\n",
       "  'line_number': 0,\n",
       "  'total_lines': 14},\n",
       " {'target': 'BACKGROUND',\n",
       "  'text': 'we tested whether theory-based education increases alarm operability .',\n",
       "  'line_number': 1,\n",
       "  'total_lines': 14},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'randomised multiarm trial , with a single arm randomly selected for use each day , in low-income neighbourhoods in maryland , usa .',\n",
       "  'line_number': 2,\n",
       "  'total_lines': 14},\n",
       " {'target': 'METHODS',\n",
       "  'text': \"intervention arms : ( @ ) full education combining a health belief module with a social-cognitive theory module that provided hands-on practice installing alarm batteries and using the alarm 's hush button ; ( @ ) hands-on practice social-cognitive module supplemented by typical fire department education ; ( @ ) current norm receiving typical fire department education only .\",\n",
       "  'line_number': 3,\n",
       "  'total_lines': 14},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'four hundred and thirty-six homes recruited through churches or by knocking on doors in @-@ .',\n",
       "  'line_number': 4,\n",
       "  'total_lines': 14},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'follow-up visits checked alarm operability in @ homes ( @ % ) @-@ @ years after installation .',\n",
       "  'line_number': 5,\n",
       "  'total_lines': 14},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'number of homes with working alarms defined as alarms with working batteries or hard-wired and number of working alarms per home .',\n",
       "  'line_number': 6,\n",
       "  'total_lines': 14},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'regressions controlled for alarm status preintervention ; demographics and beliefs about fire risks and alarm effectiveness .',\n",
       "  'line_number': 7,\n",
       "  'total_lines': 14},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'homes in the full education and practice arms were more likely to have a functioning smoke alarm at follow-up ( or = @ , @ % ci @ to @ ) and had an average of @ more working alarms per home ( @ % ci @ to @ ) .',\n",
       "  'line_number': 8,\n",
       "  'total_lines': 14},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'working alarms per home rose @ % .',\n",
       "  'line_number': 9,\n",
       "  'total_lines': 14},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'full education and practice had similar effectiveness ( p = @ on both outcome measures ) .',\n",
       "  'line_number': 10,\n",
       "  'total_lines': 14},\n",
       " {'target': 'CONCLUSIONS',\n",
       "  'text': 'without exceeding typical fire department installation time , installers can achieve greater smoke alarm operability .',\n",
       "  'line_number': 11,\n",
       "  'total_lines': 14},\n",
       " {'target': 'CONCLUSIONS',\n",
       "  'text': 'hands-on practice is key .',\n",
       "  'line_number': 12,\n",
       "  'total_lines': 14},\n",
       " {'target': 'CONCLUSIONS',\n",
       "  'text': 'two years after installation , for every three homes that received hands-on practice , one had an additional working alarm .',\n",
       "  'line_number': 13,\n",
       "  'total_lines': 14},\n",
       " {'target': 'BACKGROUND',\n",
       "  'text': 'http://www.clinicaltrials.gov number nct@ .',\n",
       "  'line_number': 14,\n",
       "  'total_lines': 14},\n",
       " {'target': 'OBJECTIVE',\n",
       "  'text': 'to evaluate the performance ( efficacy , safety and acceptability ) of a new micro-adherent absorbent dressing ( urgoclean ) compared with a hydrofiber dressing ( aquacel ) in the local management of venous leg ulcers , in the debridement stage .',\n",
       "  'line_number': 0,\n",
       "  'total_lines': 16},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'a non-inferiority european randomised controlled clinical trial ( rct ) was conducted in @ centres , on patients presenting with venous or predominantly venous , mixed aetiology leg ulcers at their sloughy stage ( with more than @ % of the wound bed covered with slough at baseline ) .',\n",
       "  'line_number': 1,\n",
       "  'total_lines': 16},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'patients were followed over a @-week period and assessed weekly .',\n",
       "  'line_number': 2,\n",
       "  'total_lines': 16},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'the primary judgement criteria was the relative regression of the wound surface area after the @-week treatment period .',\n",
       "  'line_number': 3,\n",
       "  'total_lines': 16},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'secondary endpoints were the relative reduction of sloughy tissue and the percentage of patients presenting with a debrided wound .',\n",
       "  'line_number': 4,\n",
       "  'total_lines': 16},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'altogether , @ patients were randomised to either urgoclean ( test group ; n = @ ) or aquacel ( control group ; n = @ ) dressings .',\n",
       "  'line_number': 5,\n",
       "  'total_lines': 16},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'regarding the wound healing process predictive factors ( wound area , duration , abpi value , recurrence ) , at baseline , the two groups were well balanced , for both wound and patient characteristics .',\n",
       "  'line_number': 6,\n",
       "  'total_lines': 16},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'compression therapy was administered to both groups and after a median @-day treatment period , the percentage of relative reduction of the wound surface area was very similar ( -@ % vs -@ % in the urgoclean and control groups , respectively ) .',\n",
       "  'line_number': 7,\n",
       "  'total_lines': 16},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'when considering the secondary criteria at week @ , the relative reduction of sloughy tissue was significantly higher in the urgoclean group than in the control group ( -@ % vs -@,@ % ; p = @ ) .',\n",
       "  'line_number': 8,\n",
       "  'total_lines': 16},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'the percentage of debrided wounds was also significantly higher in the test group ( @ % vs @ % ; p = @ ) .',\n",
       "  'line_number': 9,\n",
       "  'total_lines': 16},\n",
       " {'target': 'CONCLUSIONS',\n",
       "  'text': \"this ` earth ' rct confirmed that the urgoclean dressing has similar efficacy and safety compared to aquacel .\",\n",
       "  'line_number': 10,\n",
       "  'total_lines': 16},\n",
       " {'target': 'CONCLUSIONS',\n",
       "  'text': 'however , urgoclean also showed better autolytic properties than the control group in the management of venous leg ulcers at the sloughy stage .',\n",
       "  'line_number': 11,\n",
       "  'total_lines': 16}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dic = create_list_of_dic('pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt')\n",
    "my_dic[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bb104de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180040"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f491430",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git add 09_SkimLit_nlp_milestone_project_2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cb60b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\n",
      "Your branch is up to date with 'origin/master'.\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t.ipynb_checkpoints/\n",
      "\t02. Neural Network Classification with TensorFlow.ipynb\n",
      "\t03. Convolutional Neural Networks and Computer Vision with TensorFlow.ipynb\n",
      "\t04_transfer_learning_in_tensorflow_part_1_feature_extraction.ipynb\n",
      "\t05_transfer_learning_in_tensorflow_part_2_fine_tuning.ipynb\n",
      "\t\"06. Transfer Learning with TensorFlow Part 3 Scaling up (\\360\\237\\215\\224\\360\\237\\221\\201 Food Vision mini).ipynb\"\n",
      "\t06_101_food_class_10_percent_saved_big_dog_model.zip\n",
      "\t06_101_food_class_10_percent_saved_big_dog_model.zip.1\n",
      "\t06_101_food_class_10_percent_saved_big_dog_model/\n",
      "\t07_efficientnetb0_feature_extract_model_mixed_precision.zip\n",
      "\t07_efficientnetb0_feature_extract_model_mixed_precision/\n",
      "\t07_food_vision_milestone_project_1.ipynb\n",
      "\t08_introduction_to_nlp_in_tensorflow.ipynb\n",
      "\t101_classes_10_percent_data_model_checkpoint.data-00000-of-00001\n",
      "\t101_classes_10_percent_data_model_checkpoint.index\n",
      "\t101_food_classes_10_percent.zip\n",
      "\t101_food_classes_10_percent/\n",
      "\t10_food_classes_10_percent.zip\n",
      "\t10_food_classes_10_percent.zip.1\n",
      "\t10_food_classes_10_percent/\n",
      "\t10_food_classes_1_percent.zip\n",
      "\t10_food_classes_1_percent.zip.1\n",
      "\t10_food_classes_1_percent/\n",
      "\t10_food_classes_all_data/\n",
      "\tBig_dog_model/\n",
      "\tNLP-Neural-Networks-for-Joint-Sentence-Classification-in-Medical-Paper-Abstracts/\n",
      "\tTransfer_lerning_myself_test.ipynb\n",
      "\tUntitled.ipynb\n",
      "\tUntitled1.ipynb\n",
      "\tUntitled2.ipynb\n",
      "\t__MACOSX/\n",
      "\t__pycache__/\n",
      "\tcheckpoint\n",
      "\tconfusion_matrix.png\n",
      "\tdesktop.ini\n",
      "\tfood_vision_model_checkpoints.data-00000-of-00001\n",
      "\tfood_vision_model_checkpoints.index\n",
      "\thelper_functions.py\n",
      "\thelper_functions.py.1\n",
      "\thelper_functions.py.2\n",
      "\thelper_functions.py.3\n",
      "\thelper_functions.py.4\n",
      "\tmodel_checkpoints_food_vision/\n",
      "\tmodel_checkpoints_food_vision_2/\n",
      "\tmy_LSTM/\n",
      "\tmy_LSTM_3/\n",
      "\tmy_LSTM_5/\n",
      "\tmy_LSTM_7/\n",
      "\tmy_LSTM_8/\n",
      "\tmy_dir_for_twiter_1/\n",
      "\tnlp_getting_started.zip\n",
      "\tpizza_steak/\n",
      "\tpubmed-rct/\n",
      "\tsample_submission.csv\n",
      "\tsaved_trained_model_multyclass/\n",
      "\tten_percent_model_checkpoints_weights/\n",
      "\ttensorflow_hub/\n",
      "\ttesla_model_3/\n",
      "\ttest.csv\n",
      "\ttrain.csv\n",
      "\ttraining_logs/\n",
      "\ttransfer_learning/\n",
      "\ttransfer_learning_7/\n",
      "\ttransfet_lerning/\n",
      "\n",
      "nothing added to commit but untracked files present (use \"git add\" to track)\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"new commit accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7222917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_with_line_numbers(filename):\n",
    "    \"\"\"Returns a list of dictionaries of abstract line data.\n",
    "\n",
    "    Takes in filename, reads its contents and sorts through each line,\n",
    "    extracting things like the target label, the text of the sentence,\n",
    "    how many sentences are in the current abstract and what sentence number\n",
    "    the target line is.\n",
    "\n",
    "    Args:\n",
    "      filename: a string of the target text file to read and extract line data\n",
    "      from.\n",
    "\n",
    "    Returns:\n",
    "      A list of dictionaries each containing a line from an abstract,\n",
    "      the lines label, the lines position in the abstract and the total number\n",
    "      of lines in the abstract where the line is from. For example:\n",
    "\n",
    "      [{\"target\": 'CONCLUSION',\n",
    "        \"text\": The study couldn't have gone better, turns out people are kinder than you think\",\n",
    "        \"line_number\": 8,\n",
    "        \"total_lines\": 8}]\n",
    "    \"\"\"\n",
    "    input_lines = get_lines(filename) # get all lines from filename\n",
    "    abstract_lines = \"\" # create an empty abstract\n",
    "    abstract_samples = [] # create an empty list of abstracts\n",
    "\n",
    "    # Loop through each line in target file\n",
    "    for line in input_lines:\n",
    "        if line.startswith(\"###\"): # check to see if line is an ID line\n",
    "            abstract_id = line\n",
    "            abstract_lines = \"\" # reset abstract string\n",
    "        elif line.isspace(): # check to see if line is a new line\n",
    "            abstract_line_split = abstract_lines.splitlines() # split abstract into separate lines\n",
    "\n",
    "            # Iterate through each line in abstract and count them at the same time\n",
    "            for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
    "                line_data = {} # create empty dict to store data from line\n",
    "                target_text_split = abstract_line.split(\"\\t\") # split target label from text\n",
    "                line_data[\"target\"] = target_text_split[0] # get target label\n",
    "                line_data[\"text\"] = target_text_split[1].lower() # get target text and lower it\n",
    "                line_data[\"line_number\"] = abstract_line_number # what number line does the line appear in the abstract?\n",
    "                line_data[\"total_lines\"] = len(abstract_line_split) - 1 # how many total lines are in the abstract? (start from 0)\n",
    "                abstract_samples.append(line_data) # add line data to abstract samples list\n",
    "\n",
    "        else: # if the above conditions aren't fulfilled, the line contains a labelled sentence\n",
    "            abstract_lines += line\n",
    "\n",
    "    return abstract_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50c07f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\"\n",
    "train_samples = preprocess_text_with_line_numbers('pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt')\n",
    "val_samples = preprocess_text_with_line_numbers(data_dir + \"dev.txt\") # dev is another name for validation set\n",
    "test_samples = preprocess_text_with_line_numbers(data_dir + \"test.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18491a06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'target': 'OBJECTIVE',\n",
       "  'text': 'to investigate the efficacy of @ weeks of daily low-dose oral prednisolone in improving pain , mobility , and systemic low-grade inflammation in the short term and whether the effect would be sustained at @ weeks in older adults with moderate to severe knee osteoarthritis ( oa ) .',\n",
       "  'line_number': 0,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'a total of @ patients with primary knee oa were randomized @:@ ; @ received @ mg/day of prednisolone and @ received placebo for @ weeks .',\n",
       "  'line_number': 1,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'outcome measures included pain reduction and improvement in function scores and systemic inflammation markers .',\n",
       "  'line_number': 2,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'pain was assessed using the visual analog pain scale ( @-@ mm ) .',\n",
       "  'line_number': 3,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'secondary outcome measures included the western ontario and mcmaster universities osteoarthritis index scores , patient global assessment ( pga ) of the severity of knee oa , and @-min walk distance ( @mwd ) .',\n",
       "  'line_number': 4,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'serum levels of interleukin @ ( il-@ ) , il-@ , tumor necrosis factor ( tnf ) - , and high-sensitivity c-reactive protein ( hscrp ) were measured .',\n",
       "  'line_number': 5,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'there was a clinically relevant reduction in the intervention group compared to the placebo group for knee pain , physical function , pga , and @mwd at @ weeks .',\n",
       "  'line_number': 6,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'the mean difference between treatment arms ( @ % ci ) was @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; @ ( @-@ @ ) , p < @ ; and @ ( @-@ @ ) , p < @ , respectively .',\n",
       "  'line_number': 7,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'further , there was a clinically relevant reduction in the serum levels of il-@ , il-@ , tnf - , and hscrp at @ weeks in the intervention group when compared to the placebo group .',\n",
       "  'line_number': 8,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'these differences remained significant at @ weeks .',\n",
       "  'line_number': 9,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'the outcome measures in rheumatology clinical trials-osteoarthritis research society international responder rate was @ % in the intervention group and @ % in the placebo group ( p < @ ) .',\n",
       "  'line_number': 10,\n",
       "  'total_lines': 11},\n",
       " {'target': 'CONCLUSIONS',\n",
       "  'text': 'low-dose oral prednisolone had both a short-term and a longer sustained effect resulting in less knee pain , better physical function , and attenuation of systemic inflammation in older patients with knee oa ( clinicaltrials.gov identifier nct@ ) .',\n",
       "  'line_number': 11,\n",
       "  'total_lines': 11},\n",
       " {'target': 'BACKGROUND',\n",
       "  'text': 'emotional eating is associated with overeating and the development of obesity .',\n",
       "  'line_number': 0,\n",
       "  'total_lines': 10},\n",
       " {'target': 'BACKGROUND',\n",
       "  'text': 'yet , empirical evidence for individual ( trait ) differences in emotional eating and cognitive mechanisms that contribute to eating during sad mood remain equivocal .',\n",
       "  'line_number': 1,\n",
       "  'total_lines': 10},\n",
       " {'target': 'OBJECTIVE',\n",
       "  'text': 'the aim of this study was to test if attention bias for food moderates the effect of self-reported emotional eating during sad mood ( vs neutral mood ) on actual food intake .',\n",
       "  'line_number': 2,\n",
       "  'total_lines': 10},\n",
       " {'target': 'OBJECTIVE',\n",
       "  'text': 'it was expected that emotional eating is predictive of elevated attention for food and higher food intake after an experimentally induced sad mood and that attentional maintenance on food predicts food intake during a sad versus a neutral mood .',\n",
       "  'line_number': 3,\n",
       "  'total_lines': 10},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'participants ( n = @ ) were randomly assigned to one of the two experimental mood induction conditions ( sad/neutral ) .',\n",
       "  'line_number': 4,\n",
       "  'total_lines': 10},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'attentional biases for high caloric foods were measured by eye tracking during a visual probe task with pictorial food and neutral stimuli .',\n",
       "  'line_number': 5,\n",
       "  'total_lines': 10},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'self-reported emotional eating was assessed with the dutch eating behavior questionnaire ( debq ) and ad libitum food intake was tested by a disguised food offer .',\n",
       "  'line_number': 6,\n",
       "  'total_lines': 10},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'hierarchical multivariate regression modeling showed that self-reported emotional eating did not account for changes in attention allocation for food or food intake in either condition .',\n",
       "  'line_number': 7,\n",
       "  'total_lines': 10},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'yet , attention maintenance on food cues was significantly related to increased intake specifically in the neutral condition , but not in the sad mood condition .',\n",
       "  'line_number': 8,\n",
       "  'total_lines': 10},\n",
       " {'target': 'CONCLUSIONS',\n",
       "  'text': 'the current findings show that self-reported emotional eating ( based on the debq ) might not validly predict who overeats when sad , at least not in a laboratory setting with healthy women .',\n",
       "  'line_number': 9,\n",
       "  'total_lines': 10},\n",
       " {'target': 'CONCLUSIONS',\n",
       "  'text': 'results further suggest that attention maintenance on food relates to eating motivation when in a neutral affective state , and might therefore be a cognitive mechanism contributing to increased food intake in general , but maybe not during sad mood .',\n",
       "  'line_number': 10,\n",
       "  'total_lines': 10},\n",
       " {'target': 'BACKGROUND',\n",
       "  'text': 'although working smoke alarms halve deaths in residential fires , many households do not keep alarms operational .',\n",
       "  'line_number': 0,\n",
       "  'total_lines': 14},\n",
       " {'target': 'BACKGROUND',\n",
       "  'text': 'we tested whether theory-based education increases alarm operability .',\n",
       "  'line_number': 1,\n",
       "  'total_lines': 14},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'randomised multiarm trial , with a single arm randomly selected for use each day , in low-income neighbourhoods in maryland , usa .',\n",
       "  'line_number': 2,\n",
       "  'total_lines': 14},\n",
       " {'target': 'METHODS',\n",
       "  'text': \"intervention arms : ( @ ) full education combining a health belief module with a social-cognitive theory module that provided hands-on practice installing alarm batteries and using the alarm 's hush button ; ( @ ) hands-on practice social-cognitive module supplemented by typical fire department education ; ( @ ) current norm receiving typical fire department education only .\",\n",
       "  'line_number': 3,\n",
       "  'total_lines': 14},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'four hundred and thirty-six homes recruited through churches or by knocking on doors in @-@ .',\n",
       "  'line_number': 4,\n",
       "  'total_lines': 14},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'follow-up visits checked alarm operability in @ homes ( @ % ) @-@ @ years after installation .',\n",
       "  'line_number': 5,\n",
       "  'total_lines': 14},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'number of homes with working alarms defined as alarms with working batteries or hard-wired and number of working alarms per home .',\n",
       "  'line_number': 6,\n",
       "  'total_lines': 14},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'regressions controlled for alarm status preintervention ; demographics and beliefs about fire risks and alarm effectiveness .',\n",
       "  'line_number': 7,\n",
       "  'total_lines': 14},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'homes in the full education and practice arms were more likely to have a functioning smoke alarm at follow-up ( or = @ , @ % ci @ to @ ) and had an average of @ more working alarms per home ( @ % ci @ to @ ) .',\n",
       "  'line_number': 8,\n",
       "  'total_lines': 14},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'working alarms per home rose @ % .',\n",
       "  'line_number': 9,\n",
       "  'total_lines': 14},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'full education and practice had similar effectiveness ( p = @ on both outcome measures ) .',\n",
       "  'line_number': 10,\n",
       "  'total_lines': 14},\n",
       " {'target': 'CONCLUSIONS',\n",
       "  'text': 'without exceeding typical fire department installation time , installers can achieve greater smoke alarm operability .',\n",
       "  'line_number': 11,\n",
       "  'total_lines': 14},\n",
       " {'target': 'CONCLUSIONS',\n",
       "  'text': 'hands-on practice is key .',\n",
       "  'line_number': 12,\n",
       "  'total_lines': 14},\n",
       " {'target': 'CONCLUSIONS',\n",
       "  'text': 'two years after installation , for every three homes that received hands-on practice , one had an additional working alarm .',\n",
       "  'line_number': 13,\n",
       "  'total_lines': 14},\n",
       " {'target': 'BACKGROUND',\n",
       "  'text': 'http://www.clinicaltrials.gov number nct@ .',\n",
       "  'line_number': 14,\n",
       "  'total_lines': 14},\n",
       " {'target': 'OBJECTIVE',\n",
       "  'text': 'to evaluate the performance ( efficacy , safety and acceptability ) of a new micro-adherent absorbent dressing ( urgoclean ) compared with a hydrofiber dressing ( aquacel ) in the local management of venous leg ulcers , in the debridement stage .',\n",
       "  'line_number': 0,\n",
       "  'total_lines': 16},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'a non-inferiority european randomised controlled clinical trial ( rct ) was conducted in @ centres , on patients presenting with venous or predominantly venous , mixed aetiology leg ulcers at their sloughy stage ( with more than @ % of the wound bed covered with slough at baseline ) .',\n",
       "  'line_number': 1,\n",
       "  'total_lines': 16},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'patients were followed over a @-week period and assessed weekly .',\n",
       "  'line_number': 2,\n",
       "  'total_lines': 16},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'the primary judgement criteria was the relative regression of the wound surface area after the @-week treatment period .',\n",
       "  'line_number': 3,\n",
       "  'total_lines': 16},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'secondary endpoints were the relative reduction of sloughy tissue and the percentage of patients presenting with a debrided wound .',\n",
       "  'line_number': 4,\n",
       "  'total_lines': 16},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'altogether , @ patients were randomised to either urgoclean ( test group ; n = @ ) or aquacel ( control group ; n = @ ) dressings .',\n",
       "  'line_number': 5,\n",
       "  'total_lines': 16},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'regarding the wound healing process predictive factors ( wound area , duration , abpi value , recurrence ) , at baseline , the two groups were well balanced , for both wound and patient characteristics .',\n",
       "  'line_number': 6,\n",
       "  'total_lines': 16},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'compression therapy was administered to both groups and after a median @-day treatment period , the percentage of relative reduction of the wound surface area was very similar ( -@ % vs -@ % in the urgoclean and control groups , respectively ) .',\n",
       "  'line_number': 7,\n",
       "  'total_lines': 16},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'when considering the secondary criteria at week @ , the relative reduction of sloughy tissue was significantly higher in the urgoclean group than in the control group ( -@ % vs -@,@ % ; p = @ ) .',\n",
       "  'line_number': 8,\n",
       "  'total_lines': 16},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'the percentage of debrided wounds was also significantly higher in the test group ( @ % vs @ % ; p = @ ) .',\n",
       "  'line_number': 9,\n",
       "  'total_lines': 16},\n",
       " {'target': 'CONCLUSIONS',\n",
       "  'text': \"this ` earth ' rct confirmed that the urgoclean dressing has similar efficacy and safety compared to aquacel .\",\n",
       "  'line_number': 10,\n",
       "  'total_lines': 16},\n",
       " {'target': 'CONCLUSIONS',\n",
       "  'text': 'however , urgoclean also showed better autolytic properties than the control group in the management of venous leg ulcers at the sloughy stage .',\n",
       "  'line_number': 11,\n",
       "  'total_lines': 16},\n",
       " {'target': 'CONCLUSIONS',\n",
       "  'text': 'the new urgoclean dressing therefore represents a promising therapeutic option within the current range of autolytic dressings available .',\n",
       "  'line_number': 12,\n",
       "  'total_lines': 16},\n",
       " {'target': 'BACKGROUND',\n",
       "  'text': 'this study was sponsored by a grant from the pharmaceutical company laboratoires urgo .',\n",
       "  'line_number': 13,\n",
       "  'total_lines': 16},\n",
       " {'target': 'BACKGROUND',\n",
       "  'text': 's. bohbot and o. tacca are employees of laboratoires urgo .',\n",
       "  'line_number': 14,\n",
       "  'total_lines': 16},\n",
       " {'target': 'BACKGROUND',\n",
       "  'text': 's. meaume , j. dissemond and g. perceau have received monetary compensation as presenters for laboratoires urgo .',\n",
       "  'line_number': 15,\n",
       "  'total_lines': 16},\n",
       " {'target': 'BACKGROUND',\n",
       "  'text': 'data management and statistical analyses were conducted independently by vertical ( paris , france ) .',\n",
       "  'line_number': 16,\n",
       "  'total_lines': 16},\n",
       " {'target': 'OBJECTIVE',\n",
       "  'text': 'eye movements ( em ) during recall of an aversive memory is a treatment element unique to eye movement desensitization and reprocessing ( emdr ) .',\n",
       "  'line_number': 0,\n",
       "  'total_lines': 11},\n",
       " {'target': 'OBJECTIVE',\n",
       "  'text': 'experimental studies have shown that em reduce memory vividness and/or emotionality shortly after the intervention .',\n",
       "  'line_number': 1,\n",
       "  'total_lines': 11},\n",
       " {'target': 'OBJECTIVE',\n",
       "  'text': 'however , it is unclear whether the immediate effects of the intervention reflect actual changes in memory .',\n",
       "  'line_number': 2,\n",
       "  'total_lines': 11},\n",
       " {'target': 'OBJECTIVE',\n",
       "  'text': 'the aim of this study was to test whether immediate reductions in memory vividness and emotionality persist at a @h follow up and whether the magnitude of these effects is related to the duration of the intervention .',\n",
       "  'line_number': 3,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': \"seventy-three undergraduates recalled two negative autobiographical memories , one with em ( `` recall with em '' ) and one without ( `` recall only '' ) .\",\n",
       "  'line_number': 4,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'half of participants recalled each memory for four periods of @s , the other half for eight periods of @s .',\n",
       "  'line_number': 5,\n",
       "  'total_lines': 11},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'memory vividness/emotionality were self-rated at a pre-test , an immediate post-test , and a @h follow-up test .',\n",
       "  'line_number': 6,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'in both duration groups , recall with em , but not recall only , caused an immediate decrease in memory vividness .',\n",
       "  'line_number': 7,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'there were no immediate reductions in memory emotionality .',\n",
       "  'line_number': 8,\n",
       "  'total_lines': 11},\n",
       " {'target': 'RESULTS',\n",
       "  'text': \"furthermore , only the ` eight periods ' group showed that recall with em , but not recall only , caused a decrease in both memory emotionality and memory vividness from the pre-test to the follow-up .\",\n",
       "  'line_number': 9,\n",
       "  'total_lines': 11},\n",
       " {'target': 'CONCLUSIONS',\n",
       "  'text': 'only self-report measures were used .',\n",
       "  'line_number': 10,\n",
       "  'total_lines': 11},\n",
       " {'target': 'CONCLUSIONS',\n",
       "  'text': 'the findings suggest that recall with em causes @-hchanges in memory vividness/emotionality , which may explain part of the emdr treatment effect , and these effects are related to intervention duration .',\n",
       "  'line_number': 11,\n",
       "  'total_lines': 11},\n",
       " {'target': 'OBJECTIVE',\n",
       "  'text': 'few studies have tested the impact of motivational interviewing ( mi ) delivered by primary care providers on pediatric obesity .',\n",
       "  'line_number': 0,\n",
       "  'total_lines': 12},\n",
       " {'target': 'OBJECTIVE',\n",
       "  'text': 'this study tested the efficacy of mi delivered by providers and registered dietitians ( rds ) to parents of overweight children aged @ through @ .',\n",
       "  'line_number': 1,\n",
       "  'total_lines': 12},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'forty-two practices from the pediatric research in office settings network of the american academy of pediatrics were randomly assigned to @ of @ groups .',\n",
       "  'line_number': 2,\n",
       "  'total_lines': 12},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'group @ ( usual care ) measured bmi percentile at baseline and @ - and @-year follow-up .',\n",
       "  'line_number': 3,\n",
       "  'total_lines': 12},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'group @ ( provider only ) delivered @ mi counseling sessions to parents of the index child over @ years .',\n",
       "  'line_number': 4,\n",
       "  'total_lines': 12},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'group @ ( provider + rd ) delivered @ provider mi sessions plus @ mi sessions from a rd. .',\n",
       "  'line_number': 5,\n",
       "  'total_lines': 12},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'the primary outcome was child bmi percentile at @-year follow up .',\n",
       "  'line_number': 6,\n",
       "  'total_lines': 12},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'at @-year follow-up , the adjusted bmi percentile was @ , @ , and @ for groups @ , @ , and @ , respectively .',\n",
       "  'line_number': 7,\n",
       "  'total_lines': 12},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'the group @ mean was significantly ( p = @ ) lower than group @ .',\n",
       "  'line_number': 8,\n",
       "  'total_lines': 12},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'mean changes from baseline in bmi percentile were @ , @ , and @ across groups @ , @ , and @ .',\n",
       "  'line_number': 9,\n",
       "  'total_lines': 12},\n",
       " {'target': 'CONCLUSIONS',\n",
       "  'text': 'mi delivered by providers and rds ( group @ ) resulted in statistically significant reductions in bmi percentile .',\n",
       "  'line_number': 10,\n",
       "  'total_lines': 12},\n",
       " {'target': 'CONCLUSIONS',\n",
       "  'text': 'research is needed to determine the clinical significance and persistence of the bmi effects observed .',\n",
       "  'line_number': 11,\n",
       "  'total_lines': 12},\n",
       " {'target': 'CONCLUSIONS',\n",
       "  'text': 'how the intervention can be brought to scale ( in particular , how to train physicians to use mi effectively and how best to train rds and integrate them into primary care settings ) also merits future research .',\n",
       "  'line_number': 12,\n",
       "  'total_lines': 12},\n",
       " {'target': 'BACKGROUND',\n",
       "  'text': 'antithrombin ( at ) concentrations are reduced after cardiac surgery with cardiopulmonary bypass compared with the preoperative levels .',\n",
       "  'line_number': 0,\n",
       "  'total_lines': 9},\n",
       " {'target': 'BACKGROUND',\n",
       "  'text': 'low postoperative at is associated with worse short - and mid-term clinical outcomes .',\n",
       "  'line_number': 1,\n",
       "  'total_lines': 9},\n",
       " {'target': 'BACKGROUND',\n",
       "  'text': 'the aim of the study is to evaluate the effects of at administration on activation of the coagulation and fibrinolytic systems , platelet function , and the inflammatory response in patients with low postoperative at levels .',\n",
       "  'line_number': 2,\n",
       "  'total_lines': 9},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'sixty patients with postoperative at levels of less than @ % were randomly assigned to receive purified at ( @ iu in three administrations ) or placebo in the postoperative intensive care unit .',\n",
       "  'line_number': 3,\n",
       "  'total_lines': 9},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'thirty patients with postoperative at levels greater than @ % were observed as controls .',\n",
       "  'line_number': 4,\n",
       "  'total_lines': 9},\n",
       " {'target': 'METHODS',\n",
       "  'text': 'interleukin @ ( a marker of inflammation ) , prothrombin fragment @-@ ( a marker of thrombin generation ) , plasmin-antiplasmin complex ( a marker of fibrinolysis ) , and platelet factor @ ( a marker of platelet activation ) were measured at six different times .',\n",
       "  'line_number': 5,\n",
       "  'total_lines': 9},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'compared with the no at group and control patients , patients receiving at showed significantly higher at values until @ hours after the last administration .',\n",
       "  'line_number': 6,\n",
       "  'total_lines': 9},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'analysis of variance for repeated measures showed a significant effect of study treatment in reducing prothrombin fragment @-@ ( p = @ ; interaction with time sample , p = @ ) and plasmin-antiplasmin complex ( p < @ ; interaction with time sample , p < @ ) values but not interleukin @ ( p = @ ; interaction with time sample , p = @ ) and platelet factor @ ( p = @ ; interaction with time sample , p = @ ) .',\n",
       "  'line_number': 7,\n",
       "  'total_lines': 9},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'no difference in chest tube drainage , reopening for bleeding , and blood transfusion was observed .',\n",
       "  'line_number': 8,\n",
       "  'total_lines': 9},\n",
       " {'target': 'CONCLUSIONS',\n",
       "  'text': 'antithrombin administration in patients with low at activity after surgery with cardiopulmonary bypass reduces postoperative thrombin generation and fibrinolysis with no effects on platelet activation and inflammatory response .',\n",
       "  'line_number': 9,\n",
       "  'total_lines': 9},\n",
       " {'target': 'OBJECTIVE',\n",
       "  'text': 'we evaluated patients at tertiary -lsb- both percutaneous coronary intervention ( pci ) and coronary artery bypass grafting ( cabg ) capable -rsb- and primary hospitals in the early-acs trial .',\n",
       "  'line_number': 0,\n",
       "  'total_lines': 10},\n",
       " {'target': 'BACKGROUND',\n",
       "  'text': 'early invasive management is recommended for high-risk non-st-segment elevation acute coronary syndromes .',\n",
       "  'line_number': 1,\n",
       "  'total_lines': 10},\n",
       " {'target': 'METHODS',\n",
       "  'text': \"we evaluated outcomes in @,@ patients presenting to : tertiary sites , primary sites with transfer to tertiary sites ( `` transferred '' ) and those who remained at primary sites ( `` non-transfer '' ) .\",\n",
       "  'line_number': 2,\n",
       "  'total_lines': 10},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'there were @ tertiary ( n = @,@ patients ) and @ primary hospitals -lsb- n = @,@ patients ( @ transferred ; @,@ non-transfer ) -rsb- .',\n",
       "  'line_number': 3,\n",
       "  'total_lines': 10},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'significant delays occurred in time from symptom onset to angiography ( @ hr ) , pci ( @h ) , and cabg ( @ hr ) for transferred patients ( p < @ ) .',\n",
       "  'line_number': 4,\n",
       "  'total_lines': 10},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'non-transfer patients had less @-day death/myocardial infarction -lsb- @ % vs. @ % ( tertiary ) ; adjusted odds ratio ( or ) : @ ( @-@ @ ) , p = @ -rsb- ; transferred ( @ % ) and tertiary patients were similar -lsb- adjusted or : @ ( @-@ @ ) , p = @ -rsb- .',\n",
       "  'line_number': 5,\n",
       "  'total_lines': 10},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'non-transfer patients had lower @-year mortality -lsb- @ % vs. @ % ( tertiary ) ; adjusted hazard ratio ( hr ) : @ ( @-@ @ ) , p = @ -rsb- : there was no difference between transferred and tertiary patients -lsb- @ % vs. @ % ; adjusted hr : @ ( @-@ @ ) , p = @ -rsb- .',\n",
       "  'line_number': 6,\n",
       "  'total_lines': 10},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'despite similar rates of catheterization , gusto severe/moderate bleeding within @ hr was less in non-transfer -lsb- @ % vs. @ % ( tertiary ) ; adjusted or : @ ( @-@ @ ) , p < @ -rsb- , whereas transferred ( @ % ) and tertiary patients were similar -lsb- adjusted or : @ ( @-@ @ ) , p = @ -rsb- .',\n",
       "  'line_number': 7,\n",
       "  'total_lines': 10},\n",
       " {'target': 'RESULTS',\n",
       "  'text': 'there was no difference in non-cabg bleeding .',\n",
       "  'line_number': 8,\n",
       "  'total_lines': 10},\n",
       " {'target': 'CONCLUSIONS',\n",
       "  'text': 'timely angiography and revascularization were often not achieved in transferred patients .',\n",
       "  'line_number': 9,\n",
       "  'total_lines': 10}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the first abstract of our training data\n",
    "train_samples[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41a21fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180040"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "263ad87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.DataFrame(train_samples)\n",
    "val_df = pd.DataFrame(val_samples)\n",
    "test_df = pd.DataFrame(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60e6e85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "METHODS        59353\n",
       "RESULTS        57953\n",
       "CONCLUSIONS    27168\n",
       "BACKGROUND     21727\n",
       "OBJECTIVE      13839\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "085b4cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAD6CAYAAABgZXp6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAUY0lEQVR4nO3df7BfdX3n8efLAAWpCpQ0yyTQYJupS60/4Ap0tbsurhigFbpjWZi6RIcxnRF2cbozS3A6i1XZwc5WKh1lS4U1uLUh9RfZGhejYrv9gx8JoAjU4RbDkogkGn5IbWGD7/3j+wl8DffefHNyv/fL997nY+Y795z3+fH9fOZMeHHO+XzPSVUhSVIXLxl1AyRJ48sQkSR1ZohIkjozRCRJnRkikqTODBFJUmdDDZEkW5Pck+TuJJtb7agkm5I80P4e2epJcnWSySTfSnJi335WtfUfSLKqr35S2/9k2zbD7I8k6adlmL8TSbIVmKiqH/TV/hDYVVVXJlkDHFlVlyY5E/gPwJnAKcDHquqUJEcBm4EJoIAtwElV9ViS24H/CNwGbASurqovz9Smo48+upYvXz7bXZWkeWvLli0/qKrFUy07aK4bA5wNvLlNrwW+AVza6jdUL9VuTXJEkmPaupuqahdAkk3AyiTfAF5eVbe2+g3AOcCMIbJ8+XI2b948uz2SpHksyUPTLRv2PZECvpJkS5LVrbakqh5p098HlrTppcDDfdtua7WZ6tumqEuS5siwz0TeVFXbk/w8sCnJ3/UvrKpKMvTnrrQAWw1w3HHHDfvrJGnBGOqZSFVtb393AF8ATgYebZepaH93tNW3A8f2bb6s1WaqL5uiPlU7rq2qiaqaWLx4yst6kqQOhhYiSQ5P8rI908DpwLeBDcCeEVargJva9AbggjZK61TgiXbZ62bg9CRHtpFcpwM3t2VPJjm1jcq6oG9fkqQ5MMzLWUuAL7RRtwcBn6mq/53kDmB9kguBh4Bz2/ob6Y3MmgR+DLwboKp2JfkQcEdb74N7brID7wU+BRxG74b6jDfVJUmza6hDfF+MJiYmytFZkjS4JFuqamKqZf5iXZLUmSEiSerMEJEkdTaKX6xrgVi+5kudt9165Vmz2BJJw+KZiCSpM0NEktSZISJJ6swQkSR1ZohIkjozRCRJnRkikqTODBFJUmeGiCSpM0NEktSZISJJ6swQkSR1ZohIkjozRCRJnRkikqTODBFJUmeGiCSpM0NEktSZISJJ6swQkSR1ZohIkjozRCRJnRkikqTODBFJUmeGiCSpM0NEktSZISJJ6swQkSR1ZohIkjozRCRJnQ09RJIsSnJXkr9q88cnuS3JZJIbkxzS6j/T5ifb8uV9+7is1b+T5G199ZWtNplkzbD7Ikn6aXNxJnIJcH/f/EeAq6rql4DHgAtb/ULgsVa/qq1HkhOA84BfAVYCn2jBtAj4OHAGcAJwfltXkjRHhhoiSZYBZwGfbPMBTgM+21ZZC5zTps9u87Tlb2nrnw2sq6qnq+q7wCRwcvtMVtWDVfUMsK6tK0maI8M+E/lj4D8DP2nzPwc8XlW72/w2YGmbXgo8DNCWP9HWf66+1zbT1SVJc2RoIZLkN4AdVbVlWN+xH21ZnWRzks07d+4cdXMkad4Y5pnIG4G3J9lK71LTacDHgCOSHNTWWQZsb9PbgWMB2vJXAD/sr++1zXT1F6iqa6tqoqomFi9efOA9kyQBQwyRqrqsqpZV1XJ6N8a/XlW/A9wCvKOttgq4qU1vaPO05V+vqmr189roreOBFcDtwB3Aijba65D2HRuG1R9J0gsdtO9VZt2lwLokHwbuAq5r9euATyeZBHbRCwWq6t4k64H7gN3ARVX1LECSi4GbgUXA9VV175z2RJIWuDkJkar6BvCNNv0gvZFVe6/zT8BvT7P9FcAVU9Q3AhtnsamSpP3gL9YlSZ0ZIpKkzgwRSVJnhogkqTNDRJLUmSEiSerMEJEkdWaISJI6M0QkSZ0ZIpKkzgwRSVJnhogkqTNDRJLUmSEiSerMEJEkdWaISJI6M0QkSZ0ZIpKkzgwRSVJnhogkqTNDRJLUmSEiSerMEJEkdWaISJI6M0QkSZ0ZIpKkzgwRSVJnhogkqTNDRJLUmSEiSerMEJEkdWaISJI6M0QkSZ0ZIpKkzoYWIkkOTXJ7km8muTfJH7T68UluSzKZ5MYkh7T6z7T5ybZ8ed++Lmv17yR5W199ZatNJlkzrL5IkqY2UIgk+dUO+34aOK2qXgu8DliZ5FTgI8BVVfVLwGPAhW39C4HHWv2qth5JTgDOA34FWAl8IsmiJIuAjwNnACcA57d1JUlzZNAzkU+0s4r3JnnFIBtUz1Nt9uD2KeA04LOtvhY4p02f3eZpy9+SJK2+rqqerqrvApPAye0zWVUPVtUzwLq2riRpjgwUIlX168DvAMcCW5J8Jslb97VdO2O4G9gBbAL+Hni8qna3VbYBS9v0UuDh9n27gSeAn+uv77XNdHVJ0hwZ+J5IVT0A/D5wKfCvgKuT/F2SfzvDNs9W1euAZfTOHF51YM3tJsnqJJuTbN65c+comiBJ89Kg90Rek+Qq4H56l6N+s6r+eZu+al/bV9XjwC3ArwFHJDmoLVoGbG/T2+md6dCWvwL4YX99r22mq0/1/ddW1URVTSxevHif/ZUkDWbQM5E/Ae4EXltVF1XVnQBV9T16ZycvkGRxkiPa9GHAW+mF0C3AO9pqq4Cb2vSGNk9b/vWqqlY/r43eOh5YAdwO3AGsaKO9DqF3833DgP2RJM2Cg/a9CgBnAf9YVc8CJHkJcGhV/biqPj3NNscAa9soqpcA66vqr5LcB6xL8mHgLuC6tv51wKeTTAK76IUCVXVvkvXAfcBu4KK+dlwM3AwsAq6vqnv3p/OSpAMzaIh8Ffg3wJ7RVi8FvgL8i+k2qKpvAa+fov4gvfsje9f/CfjtafZ1BXDFFPWNwMZ9N1+SNAyDXs46tG+4Lm36pcNpkiRpXAwaIv+Q5MQ9M0lOAv5xOE2SJI2LQS9nvQ/4yyTfAwL8M+DfDatRkqTxMFCIVNUdSV4F/HIrfaeq/t/wmiVJGgeDnokAvAFY3rY5MQlVdcNQWiVJGgsDhUiSTwO/CNwNPNvKBRgikrSADXomMgGc0H78J0kSMPjorG/Tu5kuSdJzBj0TORq4L8nt9N4TAkBVvX0orZIkjYVBQ+QDw2yEJGk8DTrE96+T/AKwoqq+muSl9J5XJUlawAZ9FPx76L1t8E9baSnwxSG1SZI0Jga9sX4R8EbgSXjuBVU/P6xGSZLGw6Ah8nR7jznw3EujHO4rSQvcoCHy10neDxzW3q3+l8D/Gl6zJEnjYNDRWWuAC4F7gN+l9w6PTw6rUdIoLV/zpc7bbr3yrFlsifTiN+jorJ8Af9Y+kiQBgz8767tMcQ+kql456y2SJI2N/Xl21h6H0nuN7VGz3xxJ0jgZ6MZ6Vf2w77O9qv4Y8OKvJC1wg17OOrFv9iX0zkz2510kkqR5aNAg+KO+6d3AVuDcWW+NJGmsDDo6618PuyGSpPEz6OWs35tpeVV9dHaaI0kaJ/szOusNwIY2/5vA7cADw2iUJGk8DBoiy4ATq+pHAEk+AHypqt45rIZJkl78Bn121hLgmb75Z1pNkrSADXomcgNwe5IvtPlzgLVDaZEkaWwMOjrriiRfBn69ld5dVXcNr1mSpHGwPz8YfCnwZFX9jySLkxxfVd8dVsO0sPkkXWk8DPp63MuBS4HLWulg4H8Oq1GSpPEw6I313wLeDvwDQFV9D3jZsBolSRoPg4bIM1VVtMfBJzl8eE2SJI2LQUNkfZI/BY5I8h7gq/iCKkla8PYZIkkC3Ah8Fvgc8MvAf6mqP9nHdscmuSXJfUnuTXJJqx+VZFOSB9rfI/d8T5Krk0wm+Vb/k4OTrGrrP5BkVV/9pCT3tG2ubm2VJM2RfY7OqqpKsrGqfhXYtB/73g38p6q6M8nLgC1JNgHvAr5WVVcmWUPv/e2XAmcAK9rnFOAa4JQkRwGX03v0SrX9bKiqx9o67wFuo/fe95XAl/ejjZKkAzDo5aw7k7xhf3ZcVY9U1Z1t+kfA/cBS4Gye/6HiWno/XKTVb6ieW+ldOjsGeBuwqap2teDYBKxsy15eVbe2+zU39O1LkjQHBv2dyCnAO5NspTdCK/ROUl4zyMZJlgOvp3fGsKSqHmmLvs/zj09ZCjzct9m2Vpupvm2KuiRpjswYIkmOq6r/S+9soJMkP0vvXsr7qurJ/tsW7VJZdd33frRhNbAa4Ljjjhv210nSgrGvy1lfBKiqh4CPVtVD/Z997TzJwfQC5M+r6vOt/Gi7FEX7u6PVtwPH9m2+rNVmqi+bov4CVXVtVU1U1cTixYv31WxJ0oD2dTmrf7TTK/dnx22k1HXA/Xu9tGoDsAq4sv29qa9+cZJ19C6fPVFVjyS5Gfive0ZxAacDl1XVriRPJjmV3mWyC4AZR4xp/x3I40ckzX/7CpGaZnoQbwT+PXBPkrtb7f30wmN9kguBh3j+Xe0bgTOBSeDHwLsBWlh8CLijrffBqtrVpt8LfAo4jN6oLEdmSdIc2leIvDbJk/TOSA5r0/D8jfWXT7dhVf0tP30m0+8tU6xfwEXT7Ot64Pop6puBV8/YA0nS0MwYIlW1aK4aIkkaP/vzKHhpLHgfR5o7g/7YUJKkFzBEJEmdGSKSpM4MEUlSZ4aIJKkzQ0SS1JkhIknqzBCRJHVmiEiSOjNEJEmdGSKSpM4MEUlSZ4aIJKkzQ0SS1JkhIknqzBCRJHVmiEiSOjNEJEmdGSKSpM4MEUlSZ4aIJKkzQ0SS1JkhIknqzBCRJHVmiEiSOjNEJEmdHTTqBmi4lq/50qibIGke80xEktSZISJJ6swQkSR1ZohIkjozRCRJnQ1tdFaS64HfAHZU1atb7SjgRmA5sBU4t6oeSxLgY8CZwI+Bd1XVnW2bVcDvt91+uKrWtvpJwKeAw4CNwCVVVcPqzyg5wkrSi9Uwz0Q+Bazcq7YG+FpVrQC+1uYBzgBWtM9q4Bp4LnQuB04BTgYuT3Jk2+Ya4D192+39XZKkIRtaiFTV3wC79iqfDaxt02uBc/rqN1TPrcARSY4B3gZsqqpdVfUYsAlY2Za9vKpubWcfN/TtS5I0R+b6nsiSqnqkTX8fWNKmlwIP9623rdVmqm+boi5JmkMju7HeziDm5B5GktVJNifZvHPnzrn4SklaEOY6RB5tl6Jof3e0+nbg2L71lrXaTPVlU9SnVFXXVtVEVU0sXrz4gDshSeqZ6xDZAKxq06uAm/rqF6TnVOCJdtnrZuD0JEe2G+qnAze3ZU8mObWN7Lqgb1+SpDkyzCG+fwG8GTg6yTZ6o6yuBNYnuRB4CDi3rb6R3vDeSXpDfN8NUFW7knwIuKOt98Gq2nOz/r08P8T3y+0jSZpDQwuRqjp/mkVvmWLdAi6aZj/XA9dPUd8MvPpA2ihJOjD+Yl2S1JkhIknqzBCRJHVmiEiSOjNEJEmdGSKSpM6GNsRXWogO5LH9W688axZbIs0Nz0QkSZ0ZIpKkzgwRSVJnhogkqTNDRJLUmSEiSerMIb7SPODQYo2KZyKSpM48E5FeJA7kbEIaFc9EJEmdGSKSpM4MEUlSZ4aIJKkzQ0SS1JkhIknqzBCRJHVmiEiSOjNEJEmdGSKSpM4MEUlSZ4aIJKkzQ0SS1JkhIknqzBCRJHVmiEiSOvOlVNICd6Avw/L1ugubITJHfGud9EK+G378jX2IJFkJfAxYBHyyqq4ccZOkBcX/QVrYxvqeSJJFwMeBM4ATgPOTnDDaVknSwjHWIQKcDExW1YNV9QywDjh7xG2SpAVj3C9nLQUe7pvfBpwyrC/ztF168RjXf4/z7V7OuIfIQJKsBla32aeSfGeU7ZnC0cAPRt2IIZvvfbR/429O+piPDPsbpnUg/fuF6RaMe4hsB47tm1/Waj+lqq4Frp2rRu2vJJuramLU7Rim+d5H+zf+5nsfh9W/cb8ncgewIsnxSQ4BzgM2jLhNkrRgjPWZSFXtTnIxcDO9Ib7XV9W9I26WJC0YYx0iAFW1Edg46nYcoBftpbZZNN/7aP/G33zv41D6l6oaxn4lSQvAuN8TkSSNkCEyYkm2Jrknyd1JNo+6PbMhyfVJdiT5dl/tqCSbkjzQ/h45yjYeiGn694Ek29txvDvJmaNs44FIcmySW5Lcl+TeJJe0+rw4hjP0bz4dw0OT3J7km62Pf9Dqxye5LclkkhvbgKQD+y4vZ41Wkq3ARFXNmzH4Sf4l8BRwQ1W9utX+ENhVVVcmWQMcWVWXjrKdXU3Tvw8AT1XVfxtl22ZDkmOAY6rqziQvA7YA5wDvYh4cwxn6dy7z5xgGOLyqnkpyMPC3wCXA7wGfr6p1Sf478M2quuZAvsszEc26qvobYNde5bOBtW16Lb1/tGNpmv7NG1X1SFXd2aZ/BNxP7+kQ8+IYztC/eaN6nmqzB7dPAacBn231WTmGhsjoFfCVJFvaL+vnqyVV9Uib/j6wZJSNGZKLk3yrXe4ay0s9e0uyHHg9cBvz8Bju1T+YR8cwyaIkdwM7gE3A3wOPV9Xutso2ZiE8DZHRe1NVnUjvScQXtUsl81r1rqHOt+uo1wC/CLwOeAT4o5G2ZhYk+Vngc8D7qurJ/mXz4RhO0b95dQyr6tmqeh29J3mcDLxqGN9jiIxYVW1vf3cAX6B3sOejR9u16D3XpHeMuD2zqqoebf9ofwL8GWN+HNt19M8Bf15Vn2/leXMMp+rffDuGe1TV48AtwK8BRyTZ8/vAKR8Ttb8MkRFKcni7sUeSw4HTgW/PvNXY2gCsatOrgJtG2JZZt+c/rs1vMcbHsd2UvQ64v6o+2rdoXhzD6fo3z47h4iRHtOnDgLfSu/dzC/COttqsHENHZ41QklfSO/uA3tMDPlNVV4ywSbMiyV8Ab6b31NBHgcuBLwLrgeOAh4Bzq2osb05P078307sMUsBW4Hf77h+MlSRvAv4PcA/wk1Z+P737BmN/DGfo3/nMn2P4Gno3zhfRO1lYX1UfbP/NWQccBdwFvLOqnj6g7zJEJEldeTlLktSZISJJ6swQkSR1ZohIkjozRCRJnRkikqTODBFJUmeGiCSps/8PWhpjGTYZBo4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.total_lines.plot(kind =\"hist\", bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69460834",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = create_list_of_dic('pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt')\n",
    "val_samples = create_list_of_dic('pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/dev.txt')\n",
    "test_samples = create_list_of_dic('pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6506b2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f49729b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#One hot encode lables\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "train_labels_one_hot = one_hot_encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "val_labels_one_hot = one_hot_encoder.fit_transform(tf.expand_dims(val_df[\"target\"].to_numpy(), axis=-1))\n",
    "test_labels_one_hot = one_hot_encoder.fit_transform(test_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "train_labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1eb0bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 2, ..., 4, 1, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels_encoded = label_encoder.fit_transform(train_df[\"target\"].to_numpy())\n",
    "val_labels_encoded = label_encoder.transform(val_df[\"target\"].to_numpy())\n",
    "test_labels_encoded = label_encoder.transform(test_df[\"target\"].to_numpy())\n",
    "\n",
    "# Check what training labels look like\n",
    "train_labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dbb3d5c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = label_encoder.classes_\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "767fe72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>total_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>to investigate the efficacy of @ weeks of dail...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>a total of @ patients with primary knee oa wer...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>outcome measures included pain reduction and i...</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>pain was assessed using the visual analog pain...</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>secondary outcome measures included the wester...</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180035</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>for the absolute change in percent atheroma vo...</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180036</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>for pav , a significantly greater percentage o...</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180037</th>\n",
       "      <td>RESULTS</td>\n",
       "      <td>both strategies had acceptable side effect pro...</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180038</th>\n",
       "      <td>CONCLUSIONS</td>\n",
       "      <td>compared with standard statin monotherapy , th...</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180039</th>\n",
       "      <td>CONCLUSIONS</td>\n",
       "      <td>( plaque regression with cholesterol absorptio...</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180040 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             target                                               text  \\\n",
       "0         OBJECTIVE  to investigate the efficacy of @ weeks of dail...   \n",
       "1           METHODS  a total of @ patients with primary knee oa wer...   \n",
       "2           METHODS  outcome measures included pain reduction and i...   \n",
       "3           METHODS  pain was assessed using the visual analog pain...   \n",
       "4           METHODS  secondary outcome measures included the wester...   \n",
       "...             ...                                                ...   \n",
       "180035      RESULTS  for the absolute change in percent atheroma vo...   \n",
       "180036      RESULTS  for pav , a significantly greater percentage o...   \n",
       "180037      RESULTS  both strategies had acceptable side effect pro...   \n",
       "180038  CONCLUSIONS  compared with standard statin monotherapy , th...   \n",
       "180039  CONCLUSIONS  ( plaque regression with cholesterol absorptio...   \n",
       "\n",
       "        line_number  total_lines  \n",
       "0                 0           11  \n",
       "1                 1           11  \n",
       "2                 2           11  \n",
       "3                 3           11  \n",
       "4                 4           11  \n",
       "...             ...          ...  \n",
       "180035            7           11  \n",
       "180036            8           11  \n",
       "180037            9           11  \n",
       "180038           10           11  \n",
       "180039           11           11  \n",
       "\n",
       "[180040 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42110f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84a074da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([48,\n",
       "  26,\n",
       "  14,\n",
       "  13,\n",
       "  34,\n",
       "  28,\n",
       "  29,\n",
       "  54,\n",
       "  35,\n",
       "  7,\n",
       "  31,\n",
       "  38,\n",
       "  11,\n",
       "  25,\n",
       "  32,\n",
       "  39,\n",
       "  21,\n",
       "  22,\n",
       "  26,\n",
       "  25,\n",
       "  26,\n",
       "  33,\n",
       "  40,\n",
       "  17,\n",
       "  8,\n",
       "  22,\n",
       "  58,\n",
       "  15,\n",
       "  17,\n",
       "  21,\n",
       "  16,\n",
       "  50,\n",
       "  7,\n",
       "  16,\n",
       "  15,\n",
       "  4,\n",
       "  20,\n",
       "  3,\n",
       "  41,\n",
       "  49,\n",
       "  10,\n",
       "  18,\n",
       "  19,\n",
       "  28,\n",
       "  35,\n",
       "  43,\n",
       "  38,\n",
       "  24,\n",
       "  18,\n",
       "  23,\n",
       "  18,\n",
       "  13,\n",
       "  10,\n",
       "  17,\n",
       "  15,\n",
       "  25,\n",
       "  15,\n",
       "  17,\n",
       "  37,\n",
       "  27,\n",
       "  20,\n",
       "  17,\n",
       "  21,\n",
       "  8,\n",
       "  36,\n",
       "  5,\n",
       "  31,\n",
       "  20,\n",
       "  25,\n",
       "  24,\n",
       "  17,\n",
       "  20,\n",
       "  19,\n",
       "  11,\n",
       "  25,\n",
       "  15,\n",
       "  22,\n",
       "  18,\n",
       "  15,\n",
       "  38,\n",
       "  18,\n",
       "  13,\n",
       "  36,\n",
       "  33,\n",
       "  14,\n",
       "  46,\n",
       "  25,\n",
       "  86,\n",
       "  16,\n",
       "  28,\n",
       "  30,\n",
       "  12,\n",
       "  36,\n",
       "  27,\n",
       "  34,\n",
       "  59,\n",
       "  63,\n",
       "  65,\n",
       "  7,\n",
       "  11,\n",
       "  16,\n",
       "  39,\n",
       "  42,\n",
       "  36,\n",
       "  46,\n",
       "  5,\n",
       "  39,\n",
       "  63,\n",
       "  19,\n",
       "  31,\n",
       "  52,\n",
       "  15,\n",
       "  29,\n",
       "  53,\n",
       "  50,\n",
       "  18,\n",
       "  27,\n",
       "  31,\n",
       "  35,\n",
       "  39,\n",
       "  12,\n",
       "  24,\n",
       "  29,\n",
       "  22,\n",
       "  16,\n",
       "  14,\n",
       "  19,\n",
       "  49,\n",
       "  11,\n",
       "  15,\n",
       "  26,\n",
       "  23,\n",
       "  9,\n",
       "  23,\n",
       "  4,\n",
       "  13,\n",
       "  18,\n",
       "  31,\n",
       "  12,\n",
       "  20,\n",
       "  22,\n",
       "  9,\n",
       "  10,\n",
       "  16,\n",
       "  11,\n",
       "  20,\n",
       "  20,\n",
       "  14,\n",
       "  16,\n",
       "  20,\n",
       "  17,\n",
       "  39,\n",
       "  32,\n",
       "  20,\n",
       "  40,\n",
       "  20,\n",
       "  12,\n",
       "  28,\n",
       "  27,\n",
       "  12,\n",
       "  43,\n",
       "  19,\n",
       "  30,\n",
       "  7,\n",
       "  28,\n",
       "  12,\n",
       "  115,\n",
       "  26,\n",
       "  25,\n",
       "  31,\n",
       "  33,\n",
       "  16,\n",
       "  22,\n",
       "  19,\n",
       "  28,\n",
       "  10,\n",
       "  6,\n",
       "  16,\n",
       "  30,\n",
       "  42,\n",
       "  35,\n",
       "  16,\n",
       "  30,\n",
       "  16,\n",
       "  41,\n",
       "  17,\n",
       "  38,\n",
       "  28,\n",
       "  29,\n",
       "  23,\n",
       "  80,\n",
       "  18,\n",
       "  18,\n",
       "  32,\n",
       "  21,\n",
       "  10,\n",
       "  54,\n",
       "  10,\n",
       "  13,\n",
       "  27,\n",
       "  41,\n",
       "  33,\n",
       "  43,\n",
       "  14,\n",
       "  21,\n",
       "  43,\n",
       "  33,\n",
       "  28,\n",
       "  13,\n",
       "  53,\n",
       "  32,\n",
       "  79,\n",
       "  37,\n",
       "  14,\n",
       "  18,\n",
       "  29,\n",
       "  50,\n",
       "  19,\n",
       "  56,\n",
       "  12,\n",
       "  33,\n",
       "  33,\n",
       "  28,\n",
       "  21,\n",
       "  14,\n",
       "  22,\n",
       "  31,\n",
       "  10,\n",
       "  11,\n",
       "  16,\n",
       "  14,\n",
       "  3,\n",
       "  6,\n",
       "  37,\n",
       "  13,\n",
       "  9,\n",
       "  5,\n",
       "  25,\n",
       "  14,\n",
       "  59,\n",
       "  21,\n",
       "  11,\n",
       "  26,\n",
       "  15,\n",
       "  10,\n",
       "  22,\n",
       "  7,\n",
       "  16,\n",
       "  6,\n",
       "  26,\n",
       "  26,\n",
       "  34,\n",
       "  33,\n",
       "  7,\n",
       "  21,\n",
       "  26,\n",
       "  37,\n",
       "  22,\n",
       "  20,\n",
       "  15,\n",
       "  3,\n",
       "  1,\n",
       "  36,\n",
       "  19,\n",
       "  24,\n",
       "  35,\n",
       "  18,\n",
       "  25,\n",
       "  30,\n",
       "  27,\n",
       "  19,\n",
       "  20,\n",
       "  14,\n",
       "  9,\n",
       "  1,\n",
       "  42,\n",
       "  17,\n",
       "  48,\n",
       "  23,\n",
       "  18,\n",
       "  35,\n",
       "  23,\n",
       "  27,\n",
       "  36,\n",
       "  25,\n",
       "  16,\n",
       "  24,\n",
       "  56,\n",
       "  19,\n",
       "  8,\n",
       "  15,\n",
       "  9,\n",
       "  10,\n",
       "  86,\n",
       "  40,\n",
       "  44,\n",
       "  43,\n",
       "  14,\n",
       "  10,\n",
       "  16,\n",
       "  10,\n",
       "  4,\n",
       "  5,\n",
       "  30,\n",
       "  30,\n",
       "  19,\n",
       "  15,\n",
       "  12,\n",
       "  23,\n",
       "  36,\n",
       "  13,\n",
       "  27,\n",
       "  16,\n",
       "  18,\n",
       "  26,\n",
       "  16,\n",
       "  24,\n",
       "  14,\n",
       "  15,\n",
       "  20,\n",
       "  24,\n",
       "  22,\n",
       "  22,\n",
       "  6,\n",
       "  15,\n",
       "  20,\n",
       "  12,\n",
       "  32,\n",
       "  33,\n",
       "  10,\n",
       "  45,\n",
       "  51,\n",
       "  23,\n",
       "  7,\n",
       "  15,\n",
       "  21,\n",
       "  19,\n",
       "  12,\n",
       "  24,\n",
       "  29,\n",
       "  12,\n",
       "  8,\n",
       "  15,\n",
       "  21,\n",
       "  38,\n",
       "  14,\n",
       "  14,\n",
       "  23,\n",
       "  24,\n",
       "  19,\n",
       "  34,\n",
       "  25,\n",
       "  23,\n",
       "  26,\n",
       "  28,\n",
       "  29,\n",
       "  9,\n",
       "  22,\n",
       "  21,\n",
       "  34,\n",
       "  59,\n",
       "  33,\n",
       "  14,\n",
       "  10,\n",
       "  29,\n",
       "  49,\n",
       "  17,\n",
       "  72,\n",
       "  7,\n",
       "  12,\n",
       "  24,\n",
       "  13,\n",
       "  13,\n",
       "  6,\n",
       "  45,\n",
       "  12,\n",
       "  19,\n",
       "  21,\n",
       "  29,\n",
       "  20,\n",
       "  21,\n",
       "  16,\n",
       "  29,\n",
       "  9,\n",
       "  16,\n",
       "  44,\n",
       "  19,\n",
       "  13,\n",
       "  8,\n",
       "  22,\n",
       "  13,\n",
       "  11,\n",
       "  25,\n",
       "  13,\n",
       "  26,\n",
       "  14,\n",
       "  32,\n",
       "  15,\n",
       "  11,\n",
       "  14,\n",
       "  21,\n",
       "  14,\n",
       "  29,\n",
       "  35,\n",
       "  29,\n",
       "  18,\n",
       "  29,\n",
       "  20,\n",
       "  36,\n",
       "  22,\n",
       "  28,\n",
       "  16,\n",
       "  59,\n",
       "  14,\n",
       "  22,\n",
       "  44,\n",
       "  28,\n",
       "  19,\n",
       "  21,\n",
       "  6,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  18,\n",
       "  11,\n",
       "  17,\n",
       "  14,\n",
       "  28,\n",
       "  21,\n",
       "  20,\n",
       "  17,\n",
       "  10,\n",
       "  13,\n",
       "  74,\n",
       "  37,\n",
       "  42,\n",
       "  24,\n",
       "  9,\n",
       "  11,\n",
       "  24,\n",
       "  11,\n",
       "  22,\n",
       "  35,\n",
       "  58,\n",
       "  6,\n",
       "  119,\n",
       "  16,\n",
       "  42,\n",
       "  8,\n",
       "  36,\n",
       "  41,\n",
       "  14,\n",
       "  16,\n",
       "  25,\n",
       "  34,\n",
       "  16,\n",
       "  29,\n",
       "  18,\n",
       "  22,\n",
       "  43,\n",
       "  11,\n",
       "  24,\n",
       "  10,\n",
       "  24,\n",
       "  37,\n",
       "  25,\n",
       "  55,\n",
       "  41,\n",
       "  24,\n",
       "  37,\n",
       "  21,\n",
       "  18,\n",
       "  10,\n",
       "  10,\n",
       "  26,\n",
       "  33,\n",
       "  19,\n",
       "  23,\n",
       "  9,\n",
       "  29,\n",
       "  26,\n",
       "  29,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  20,\n",
       "  32,\n",
       "  32,\n",
       "  11,\n",
       "  34,\n",
       "  21,\n",
       "  7,\n",
       "  17,\n",
       "  9,\n",
       "  9,\n",
       "  5,\n",
       "  14,\n",
       "  77,\n",
       "  14,\n",
       "  31,\n",
       "  26,\n",
       "  5,\n",
       "  26,\n",
       "  44,\n",
       "  15,\n",
       "  32,\n",
       "  45,\n",
       "  17,\n",
       "  16,\n",
       "  13,\n",
       "  26,\n",
       "  22,\n",
       "  12,\n",
       "  21,\n",
       "  20,\n",
       "  10,\n",
       "  19,\n",
       "  26,\n",
       "  25,\n",
       "  42,\n",
       "  16,\n",
       "  10,\n",
       "  14,\n",
       "  29,\n",
       "  16,\n",
       "  31,\n",
       "  86,\n",
       "  17,\n",
       "  19,\n",
       "  22,\n",
       "  24,\n",
       "  11,\n",
       "  21,\n",
       "  21,\n",
       "  27,\n",
       "  51,\n",
       "  28,\n",
       "  22,\n",
       "  29,\n",
       "  66,\n",
       "  46,\n",
       "  24,\n",
       "  18,\n",
       "  31,\n",
       "  34,\n",
       "  34,\n",
       "  25,\n",
       "  26,\n",
       "  26,\n",
       "  18,\n",
       "  12,\n",
       "  26,\n",
       "  26,\n",
       "  25,\n",
       "  32,\n",
       "  21,\n",
       "  39,\n",
       "  24,\n",
       "  16,\n",
       "  8,\n",
       "  7,\n",
       "  37,\n",
       "  48,\n",
       "  29,\n",
       "  58,\n",
       "  53,\n",
       "  23,\n",
       "  25,\n",
       "  12,\n",
       "  21,\n",
       "  22,\n",
       "  52,\n",
       "  13,\n",
       "  77,\n",
       "  33,\n",
       "  39,\n",
       "  51,\n",
       "  25,\n",
       "  42,\n",
       "  15,\n",
       "  18,\n",
       "  26,\n",
       "  13,\n",
       "  20,\n",
       "  29,\n",
       "  7,\n",
       "  39,\n",
       "  18,\n",
       "  12,\n",
       "  18,\n",
       "  33,\n",
       "  7,\n",
       "  22,\n",
       "  31,\n",
       "  8,\n",
       "  36,\n",
       "  14,\n",
       "  84,\n",
       "  35,\n",
       "  44,\n",
       "  61,\n",
       "  110,\n",
       "  110,\n",
       "  64,\n",
       "  24,\n",
       "  21,\n",
       "  31,\n",
       "  13,\n",
       "  50,\n",
       "  46,\n",
       "  16,\n",
       "  41,\n",
       "  15,\n",
       "  9,\n",
       "  18,\n",
       "  32,\n",
       "  19,\n",
       "  23,\n",
       "  23,\n",
       "  27,\n",
       "  41,\n",
       "  22,\n",
       "  22,\n",
       "  19,\n",
       "  19,\n",
       "  20,\n",
       "  15,\n",
       "  34,\n",
       "  9,\n",
       "  29,\n",
       "  10,\n",
       "  13,\n",
       "  16,\n",
       "  35,\n",
       "  25,\n",
       "  29,\n",
       "  31,\n",
       "  5,\n",
       "  12,\n",
       "  25,\n",
       "  47,\n",
       "  41,\n",
       "  22,\n",
       "  31,\n",
       "  24,\n",
       "  13,\n",
       "  33,\n",
       "  41,\n",
       "  13,\n",
       "  19,\n",
       "  34,\n",
       "  24,\n",
       "  5,\n",
       "  21,\n",
       "  25,\n",
       "  54,\n",
       "  27,\n",
       "  15,\n",
       "  29,\n",
       "  44,\n",
       "  11,\n",
       "  69,\n",
       "  20,\n",
       "  22,\n",
       "  4,\n",
       "  23,\n",
       "  52,\n",
       "  15,\n",
       "  13,\n",
       "  58,\n",
       "  36,\n",
       "  24,\n",
       "  28,\n",
       "  15,\n",
       "  6,\n",
       "  22,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  22,\n",
       "  41,\n",
       "  32,\n",
       "  33,\n",
       "  20,\n",
       "  23,\n",
       "  17,\n",
       "  13,\n",
       "  26,\n",
       "  56,\n",
       "  11,\n",
       "  53,\n",
       "  38,\n",
       "  37,\n",
       "  50,\n",
       "  35,\n",
       "  22,\n",
       "  30,\n",
       "  62,\n",
       "  12,\n",
       "  22,\n",
       "  21,\n",
       "  18,\n",
       "  10,\n",
       "  17,\n",
       "  20,\n",
       "  29,\n",
       "  13,\n",
       "  12,\n",
       "  11,\n",
       "  17,\n",
       "  8,\n",
       "  16,\n",
       "  26,\n",
       "  15,\n",
       "  37,\n",
       "  9,\n",
       "  12,\n",
       "  16,\n",
       "  13,\n",
       "  12,\n",
       "  24,\n",
       "  9,\n",
       "  19,\n",
       "  23,\n",
       "  31,\n",
       "  27,\n",
       "  61,\n",
       "  34,\n",
       "  41,\n",
       "  19,\n",
       "  15,\n",
       "  11,\n",
       "  27,\n",
       "  22,\n",
       "  18,\n",
       "  20,\n",
       "  33,\n",
       "  25,\n",
       "  13,\n",
       "  11,\n",
       "  8,\n",
       "  17,\n",
       "  18,\n",
       "  20,\n",
       "  44,\n",
       "  15,\n",
       "  23,\n",
       "  69,\n",
       "  31,\n",
       "  32,\n",
       "  28,\n",
       "  27,\n",
       "  31,\n",
       "  23,\n",
       "  25,\n",
       "  36,\n",
       "  29,\n",
       "  15,\n",
       "  19,\n",
       "  14,\n",
       "  28,\n",
       "  9,\n",
       "  34,\n",
       "  14,\n",
       "  15,\n",
       "  24,\n",
       "  37,\n",
       "  19,\n",
       "  29,\n",
       "  21,\n",
       "  43,\n",
       "  63,\n",
       "  39,\n",
       "  86,\n",
       "  6,\n",
       "  26,\n",
       "  10,\n",
       "  15,\n",
       "  29,\n",
       "  19,\n",
       "  17,\n",
       "  42,\n",
       "  12,\n",
       "  58,\n",
       "  37,\n",
       "  28,\n",
       "  33,\n",
       "  32,\n",
       "  3,\n",
       "  27,\n",
       "  20,\n",
       "  2,\n",
       "  17,\n",
       "  28,\n",
       "  22,\n",
       "  27,\n",
       "  17,\n",
       "  17,\n",
       "  13,\n",
       "  19,\n",
       "  29,\n",
       "  6,\n",
       "  13,\n",
       "  2,\n",
       "  38,\n",
       "  19,\n",
       "  20,\n",
       "  34,\n",
       "  9,\n",
       "  21,\n",
       "  20,\n",
       "  18,\n",
       "  35,\n",
       "  38,\n",
       "  1,\n",
       "  21,\n",
       "  23,\n",
       "  25,\n",
       "  10,\n",
       "  7,\n",
       "  52,\n",
       "  40,\n",
       "  33,\n",
       "  26,\n",
       "  37,\n",
       "  31,\n",
       "  43,\n",
       "  31,\n",
       "  33,\n",
       "  47,\n",
       "  35,\n",
       "  35,\n",
       "  36,\n",
       "  11,\n",
       "  21,\n",
       "  21,\n",
       "  12,\n",
       "  15,\n",
       "  11,\n",
       "  13,\n",
       "  11,\n",
       "  25,\n",
       "  21,\n",
       "  29,\n",
       "  23,\n",
       "  23,\n",
       "  29,\n",
       "  45,\n",
       "  26,\n",
       "  26,\n",
       "  26,\n",
       "  27,\n",
       "  36,\n",
       "  33,\n",
       "  13,\n",
       "  20,\n",
       "  38,\n",
       "  15,\n",
       "  25,\n",
       "  20,\n",
       "  7,\n",
       "  23,\n",
       "  25,\n",
       "  7,\n",
       "  80,\n",
       "  35,\n",
       "  11,\n",
       "  18,\n",
       "  20,\n",
       "  18,\n",
       "  42,\n",
       "  18,\n",
       "  13,\n",
       "  34,\n",
       "  20,\n",
       "  20,\n",
       "  41,\n",
       "  37,\n",
       "  33,\n",
       "  26,\n",
       "  20,\n",
       "  31,\n",
       "  24,\n",
       "  27,\n",
       "  22,\n",
       "  30,\n",
       "  47,\n",
       "  83,\n",
       "  18,\n",
       "  21,\n",
       "  21,\n",
       "  32,\n",
       "  23,\n",
       "  29,\n",
       "  14,\n",
       "  23,\n",
       "  26,\n",
       "  10,\n",
       "  18,\n",
       "  17,\n",
       "  20,\n",
       "  54,\n",
       "  49,\n",
       "  31,\n",
       "  39,\n",
       "  39,\n",
       "  16,\n",
       "  16,\n",
       "  19,\n",
       "  49,\n",
       "  5,\n",
       "  11,\n",
       "  25,\n",
       "  19,\n",
       "  16,\n",
       "  7,\n",
       "  10,\n",
       "  13,\n",
       "  38,\n",
       "  46,\n",
       "  33,\n",
       "  31,\n",
       "  21,\n",
       "  40,\n",
       "  76,\n",
       "  26,\n",
       "  25,\n",
       "  7,\n",
       "  9,\n",
       "  33,\n",
       "  27,\n",
       "  26,\n",
       "  29,\n",
       "  31,\n",
       "  31,\n",
       "  64,\n",
       "  24,\n",
       "  17,\n",
       "  13,\n",
       "  36,\n",
       "  8,\n",
       "  75,\n",
       "  11,\n",
       "  21,\n",
       "  34,\n",
       "  22,\n",
       "  7,\n",
       "  18,\n",
       "  18,\n",
       "  16,\n",
       "  10,\n",
       "  32,\n",
       "  7,\n",
       "  10,\n",
       "  14,\n",
       "  23,\n",
       "  26,\n",
       "  5,\n",
       "  15,\n",
       "  58,\n",
       "  33,\n",
       "  23,\n",
       "  9,\n",
       "  36,\n",
       "  50,\n",
       "  10,\n",
       "  11,\n",
       "  21,\n",
       "  18,\n",
       "  14,\n",
       "  18,\n",
       "  22,\n",
       "  12,\n",
       "  24,\n",
       "  8,\n",
       "  28,\n",
       "  15,\n",
       "  6,\n",
       "  28,\n",
       "  23,\n",
       "  24,\n",
       "  21,\n",
       "  19,\n",
       "  13,\n",
       "  32,\n",
       "  10,\n",
       "  10,\n",
       "  20,\n",
       "  17,\n",
       "  49,\n",
       "  34,\n",
       "  21,\n",
       "  33,\n",
       "  62,\n",
       "  22,\n",
       "  52,\n",
       "  18,\n",
       "  11,\n",
       "  14,\n",
       "  16,\n",
       "  ...],\n",
       " 25.338269273494777)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_num = []\n",
    "for index in train_df.index:\n",
    "    list_of_num.append(train_df['text'][index].count(' '))\n",
    "    \n",
    "avg = mean(list_of_num)\n",
    "list_of_num, avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f721d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_hist = pd.DataFrame(list_of_num)\n",
    "df_hist.plot.hist(bins=200, alpha=0.7, figsize=(10, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae344138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "# Setup text vectorization with custom variables\n",
    "max_vocab_length = 86000 # max number of words to have in our vocabulary\n",
    "max_length = 60 # max length our sequences will be (e.g. how many words from a Tweet does our model see?)\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
    "                                    output_mode=\"int\",\n",
    "                                    output_sequence_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6766896b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer.adapt(train_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ee8b7b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(60,), dtype=int64, numpy=\n",
       "array([  299,    20,   784,  2628,    18,   457, 39644,     5,    71,\n",
       "           4,  1046,   265,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0], dtype=int64)>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer(train_df['text'][10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e32da52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 64841\n",
      "Top 5 most common words: ['', '[UNK]', 'the', 'and', 'of']\n",
      "Bottom 5 least common words: ['aainduced', 'aaigroup', 'aachener', 'aachen', 'aaacp']\n"
     ]
    }
   ],
   "source": [
    "# Get the unique words in the vocabulary\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "top_5_words = words_in_vocab[:5] # most common tokens (notice the [UNK] token for \"unknown\" words)\n",
    "bottom_5_words = words_in_vocab[-5:] # least common tokens\n",
    "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
    "print(f\"Top 5 most common words: {top_5_words}\") \n",
    "print(f\"Bottom 5 least common words: {bottom_5_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "31af4792",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "embedding = layers.Embedding(input_dim=max_vocab_length, # set input shape\n",
    "                             output_dim=128, # set size of embedding vector\n",
    "                             embeddings_initializer=\"uniform\", # default, intialize randomly\n",
    "                             mask_zero=True,\n",
    "                             input_length=max_length, # how long is each input\n",
    "                             name=\"embedding_1\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cfc4cfe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       "array([ 0.03748605, -0.00021989, -0.04359752,  0.02412561, -0.00687733,\n",
       "       -0.00643288,  0.04683146, -0.03231154,  0.01906149,  0.00058554,\n",
       "       -0.03832903, -0.03910276, -0.04623865, -0.04948337,  0.01483753,\n",
       "        0.03817955, -0.04513848, -0.03937522, -0.01187236, -0.00577147,\n",
       "        0.03582468, -0.0335082 , -0.02434157,  0.0161026 ,  0.03354938,\n",
       "       -0.03796289,  0.0343806 , -0.02395328, -0.01672857,  0.02602958,\n",
       "        0.02377399,  0.03455451, -0.01289757, -0.03740299,  0.01408248,\n",
       "        0.01645428, -0.02191278,  0.02893208, -0.03673612, -0.03886055,\n",
       "       -0.02875739,  0.008676  , -0.03322427,  0.00662416,  0.00385163,\n",
       "       -0.03722645, -0.00656782,  0.03742001,  0.03201741, -0.02266761,\n",
       "        0.01882419,  0.00304293, -0.04187814,  0.01398506, -0.02172558,\n",
       "        0.02978731, -0.00608387, -0.03582561, -0.01819932,  0.02611626,\n",
       "       -0.03005253, -0.04457638, -0.03126935,  0.04606754,  0.0299225 ,\n",
       "       -0.00342657, -0.01053929, -0.03433595, -0.02961108, -0.03808407,\n",
       "       -0.03960342, -0.01128981,  0.02323544,  0.00897694,  0.04796406,\n",
       "       -0.03082354, -0.00990561,  0.03375633, -0.00232512,  0.02561359,\n",
       "        0.02840805, -0.02767366,  0.00866773, -0.02851802,  0.00066904,\n",
       "        0.01075522,  0.00350785,  0.02143717,  0.02940111, -0.03470248,\n",
       "        0.03902492,  0.0063001 ,  0.04769714,  0.01361766,  0.03502854,\n",
       "        0.00769162, -0.01836876,  0.01172452,  0.00593518, -0.02484953,\n",
       "       -0.02172772,  0.02130708,  0.02185449,  0.04992019, -0.00293899,\n",
       "       -0.01645871, -0.01881897,  0.02707623, -0.04264832,  0.02238416,\n",
       "       -0.04463668, -0.00600437,  0.04105082,  0.03135702,  0.0490644 ,\n",
       "        0.03259994,  0.01245366, -0.00730918,  0.04863559,  0.03251395,\n",
       "       -0.01434626,  0.00468625,  0.03306634, -0.00377696, -0.02785997,\n",
       "       -0.03495694,  0.01603654, -0.01352614], dtype=float32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_embed = embedding(text_vectorizer(train_df['text'][10000]))\n",
    "sample_embed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "539ea5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6fd9efc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_conv\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVec  (None, 60)               0         \n",
      " torization)                                                     \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 60, 128)           11008000  \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 58, 32)            12320     \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 32)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 165       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,020,485\n",
      "Trainable params: 11,020,485\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype='string')\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.Conv1D(filters=32, kernel_size=3, activation='relu')(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(5, activation='softmax')(x)\n",
    "model_1 = tf.keras.Model(inputs, outputs, name='model_1_conv')\n",
    "\n",
    "model_1.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "               optimizer=tf.keras.optimizers.Adam(),\n",
    "               metrics=[\"accuracy\"])\n",
    "\n",
    "# Get a summary of our 1D convolution model\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c72b8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5627/5627 [==============================] - 53s 9ms/step - loss: 0.5217 - accuracy: 0.8146 - val_loss: 0.5337 - val_accuracy: 0.8097\n",
      "Epoch 2/5\n",
      "5627/5627 [==============================] - 51s 9ms/step - loss: 0.4249 - accuracy: 0.8528 - val_loss: 0.5399 - val_accuracy: 0.8079\n",
      "Epoch 3/5\n",
      "5627/5627 [==============================] - 52s 9ms/step - loss: 0.3571 - accuracy: 0.8787 - val_loss: 0.5686 - val_accuracy: 0.8060\n",
      "Epoch 4/5\n",
      "5627/5627 [==============================] - 51s 9ms/step - loss: 0.3057 - accuracy: 0.8970 - val_loss: 0.6048 - val_accuracy: 0.8031\n",
      "Epoch 5/5\n",
      "5627/5627 [==============================] - 50s 9ms/step - loss: 0.2634 - accuracy: 0.9122 - val_loss: 0.6488 - val_accuracy: 0.7939\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_1_history = model_1.fit(train_df['text'],\n",
    "                              train_labels_one_hot,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_df['text'], val_labels_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa28f6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "942/942 [==============================] - 3s 3ms/step - loss: 0.6738 - accuracy: 0.7854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6737927198410034, 0.785365879535675]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(test_df['text'], test_labels_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "536669aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = train_df[\"text\"].tolist()\n",
    "val_sentences = val_df[\"text\"].tolist()\n",
    "test_sentences = test_df[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "18a97aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(5,), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn our data into TensorFlow Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_one_hot))\n",
    "\n",
    "train_dataset\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6d452feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the TensorSliceDataset's and turn them into prefetched batches\n",
    "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3aedfb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "562/562 [==============================] - 7s 12ms/step - loss: 0.6475 - accuracy: 0.7794 - val_loss: 0.6035 - val_accuracy: 0.7858\n",
      "Epoch 2/5\n",
      "562/562 [==============================] - 7s 12ms/step - loss: 0.3913 - accuracy: 0.8687 - val_loss: 0.5922 - val_accuracy: 0.7947\n",
      "Epoch 3/5\n",
      "562/562 [==============================] - 7s 12ms/step - loss: 0.3503 - accuracy: 0.8835 - val_loss: 0.5947 - val_accuracy: 0.7944\n",
      "Epoch 4/5\n",
      "562/562 [==============================] - 7s 12ms/step - loss: 0.3343 - accuracy: 0.8900 - val_loss: 0.6008 - val_accuracy: 0.7961\n",
      "Epoch 5/5\n",
      "562/562 [==============================] - 7s 12ms/step - loss: 0.3299 - accuracy: 0.8948 - val_loss: 0.6013 - val_accuracy: 0.7979\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "inputs = layers.Input(shape=(1,), dtype='string')\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.Conv1D(filters=32, kernel_size=3, activation='relu')(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(5, activation='softmax')(x)\n",
    "model_2 = tf.keras.Model(inputs, outputs, name='model_1_conv')\n",
    "\n",
    "model_2.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "               optimizer=tf.keras.optimizers.Adam(),\n",
    "               metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit the model\n",
    "model_2_history = model_2.fit(train_dataset,\n",
    "                              steps_per_epoch=int(0.1*len(train_dataset)),\n",
    "                              epochs=5,\n",
    "                              validation_data=valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "61c5d88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.1705042e-01, 2.0391105e-03, 4.6073057e-02, 7.2288578e-03,\n",
       "        2.7608426e-02],\n",
       "       [7.3197752e-01, 3.9534863e-02, 3.6745198e-04, 2.2644351e-01,\n",
       "        1.6766295e-03],\n",
       "       [5.1296692e-02, 4.7058120e-06, 6.1191648e-05, 9.4863743e-01,\n",
       "        7.2206099e-09],\n",
       "       ...,\n",
       "       [1.5631640e-10, 5.6528243e-08, 5.2601932e-05, 5.2882494e-09,\n",
       "        9.9994731e-01],\n",
       "       [7.5906076e-02, 5.8147347e-01, 2.7459818e-01, 1.2506004e-03,\n",
       "        6.6771738e-02],\n",
       "       [3.4913570e-05, 9.9990952e-01, 4.4559474e-06, 2.7702589e-08,\n",
       "        5.0974908e-05]], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions (our model outputs prediction probabilities for each class)\n",
    "model_1_pred_probs = model_1.predict(valid_dataset)\n",
    "model_1_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5c20589e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 0, 3, ..., 4, 1, 1], dtype=int64)>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert pred probs to classes\n",
    "model_1_preds = tf.argmax(model_1_pred_probs, axis=1)\n",
    "model_1_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e488c6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pretrained TensorFlow Hub USE\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bebe2521",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_hub_embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        trainable=False, \n",
    "                                        name=\"universal_sentence_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b0ab0446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "562/562 [==============================] - 13s 21ms/step - loss: 0.9165 - accuracy: 0.6483 - val_loss: 0.7965 - val_accuracy: 0.6892\n",
      "Epoch 2/5\n",
      "562/562 [==============================] - 11s 19ms/step - loss: 0.7684 - accuracy: 0.7017 - val_loss: 0.7546 - val_accuracy: 0.7088\n",
      "Epoch 3/5\n",
      "562/562 [==============================] - 10s 19ms/step - loss: 0.7503 - accuracy: 0.7127 - val_loss: 0.7382 - val_accuracy: 0.7138\n",
      "Epoch 4/5\n",
      "562/562 [==============================] - 10s 18ms/step - loss: 0.7167 - accuracy: 0.7255 - val_loss: 0.7096 - val_accuracy: 0.7344\n",
      "Epoch 5/5\n",
      "562/562 [==============================] - 10s 18ms/step - loss: 0.7242 - accuracy: 0.7226 - val_loss: 0.6876 - val_accuracy: 0.7370\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cdd1f23af0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = layers.Input(shape=[], dtype=tf.string)\n",
    "pretrained_embeding = tf_hub_embedding_layer(inputs)\n",
    "x = layers.Dense(128, activation='relu')(pretrained_embeding)\n",
    "outputs = layers.Dense(5, activation=\"softmax\")(x)\n",
    "model_3 = tf.keras.Model(inputs=inputs,\n",
    "                        outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model_3.compile(loss=\"categorical_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])\n",
    "# Fit feature extractor model for 3 epochs\n",
    "model_3.fit(train_dataset,\n",
    "            steps_per_epoch=int(0.1 * len(train_dataset)),\n",
    "            epochs=5,\n",
    "            validation_data=valid_dataset,\n",
    "            validation_steps=int(0.1 * len(valid_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c3484321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945/945 [==============================] - 14s 15ms/step - loss: 0.6954 - accuracy: 0.7365\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6954176425933838, 0.7365285158157349]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on whole validation dataset\n",
    "model_3.evaluate(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "baa91f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.3633370e-01, 2.4286996e-01, 2.3133846e-03, 1.0941595e-01,\n",
       "        9.0670111e-03],\n",
       "       [4.7434354e-01, 4.2650515e-01, 6.3392390e-03, 9.0334103e-02,\n",
       "        2.4779139e-03],\n",
       "       [4.0480494e-01, 7.2593831e-02, 4.3561865e-02, 4.4229883e-01,\n",
       "        3.6740467e-02],\n",
       "       ...,\n",
       "       [1.9185670e-03, 1.5742304e-03, 1.1529124e-01, 4.9790140e-04,\n",
       "        8.8071805e-01],\n",
       "       [2.1656067e-03, 2.1227453e-02, 4.9046493e-01, 8.1622432e-04,\n",
       "        4.8532578e-01],\n",
       "       [1.5635285e-01, 3.7014985e-01, 4.2191091e-01, 2.9011325e-03,\n",
       "        4.8685245e-02]], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions with feature extraction model\n",
    "model_3_pred_probs = model_3.predict(valid_dataset)\n",
    "model_3_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b23f6754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 0, 3, ..., 4, 2, 2], dtype=int64)>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the predictions with feature extraction model to classes\n",
    "model_3_preds = tf.argmax(model_3_pred_probs, axis=1)\n",
    "model_3_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "24d681a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import calculate_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9de35490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 73.65285317092545,\n",
       " 'precision': 0.7330888535474944,\n",
       " 'recall': 0.7365285317092546,\n",
       " 'f1': 0.7309108989230092}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate results from TF Hub pretrained embeddings results on validation set\n",
    "model_3_results = calculate_results(y_true=val_labels_encoded,\n",
    "                                    y_pred=model_3_preds)\n",
    "model_3_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "57eafe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random_training_sentence = random.choice(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "df7e93c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_chars(text):\n",
    "    return \" \".join(list(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "de8b1d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a l t h o u g h   c f p   i s   a   u s e f u l   m e t h o d   f o r   e x t e n s i v e   m a x i l l a r y   s i n u s   d i s e a s e   ,   c o m p l i c a t i o n s   c a n   o c c u r   a s s o c i a t e d   w i t h   n e r v e   i n j u r i e s   o r   m u c o s a l   b l e e d i n g   .'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test splitting non-character-level sequence into characters\n",
    "split_chars(random_training_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "74f34ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t h e   o u t c o m e   m e a s u r e s   i n   r h e u m a t o l o g y   c l i n i c a l   t r i a l s - o s t e o a r t h r i t i s   r e s e a r c h   s o c i e t y   i n t e r n a t i o n a l   r e s p o n d e r   r a t e   w a s   @   %   i n   t h e   i n t e r v e n t i o n   g r o u p   a n d   @   %   i n   t h e   p l a c e b o   g r o u p   (   p   <   @   )   .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split sequence-level data splits into character-level data splits\n",
    "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
    "val_chars = [split_chars(sentence) for sentence in val_sentences]\n",
    "test_chars = [split_chars(sentence) for sentence in test_sentences]\n",
    "print(train_chars[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "28d7601e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149.3662574983337"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# What's the average character length?\n",
    "char_lens = [len(sentence) for sentence in train_sentences]\n",
    "mean_char_len = np.mean(char_lens)\n",
    "mean_char_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4d5d4419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1224x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(17, 12))\n",
    "plt.hist(char_lens, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b8154876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find what character length covers 95% of sequences\n",
    "output_seq_char_len = int(np.percentile(char_lens, 95))\n",
    "output_seq_char_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "508a3c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyz0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all keyboard characters for char-level embedding\n",
    "import string\n",
    "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
    "alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "10bd314c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alphabet) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "930f1369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create char-level token vectorizer instance\n",
    "NUM_CHAR_TOKENS = len(alphabet) + 2 # num characters in alphabet + space + OOV token\n",
    "char_vectorizer = TextVectorization(max_tokens=NUM_CHAR_TOKENS,  \n",
    "                                    output_sequence_length=output_seq_char_len,\n",
    "                                    standardize=\"lower_and_strip_punctuation\",\n",
    "                                    name=\"char_vectorizer\")\n",
    "\n",
    "# Adapt character vectorizer to training characters\n",
    "char_vectorizer.adapt(train_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4fe84800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different characters in character vocab: 28\n",
      "5 most common characters: ['', '[UNK]', 'e', 't', 'i']\n",
      "5 least common characters: ['k', 'x', 'z', 'q', 'j']\n"
     ]
    }
   ],
   "source": [
    "# Check character vocabulary characteristics\n",
    "char_vocab = char_vectorizer.get_vocabulary()\n",
    "print(f\"Number of different characters in character vocab: {len(char_vocab)}\")\n",
    "print(f\"5 most common characters: {char_vocab[:5]}\")\n",
    "print(f\"5 least common characters: {char_vocab[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "460a9e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charified text:\n",
      "i n   p a s i r e o t i d e   l a r   a n d   o c t r e o t i d e   l a r   p a t i e n t s   ,   r e s p e c t i v e l y   ,   @   %   a n d   @   %   (   p   =   @   )   a c h i e v e d   n o r m a l   i g f - @   ,   a n d   @   %   a n d   @   %   a c h i e v e d   g h   <   @   g / l   .\n",
      "\n",
      "Length of chars: 112\n",
      "\n",
      "Vectorized chars:\n",
      "[[ 4  6 14  5  9  4  8  2  7  3  4 10  2 12  5  8  5  6 10  7 11  3  8  2\n",
      "   7  3  4 10  2 12  5  8 14  5  3  4  2  6  3  9  8  2  9 14  2 11  3  4\n",
      "  21  2 12 19  5  6 10 14  5 11 13  4  2 21  2 10  6  7  8 15  5 12  4 18\n",
      "  17  5  6 10  5  6 10  5 11 13  4  2 21  2 10 18 13 18 12  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]]\n",
      "\n",
      "Length of vectorized chars: 290\n"
     ]
    }
   ],
   "source": [
    "# Test out character vectorizer\n",
    "random_train_chars = random.choice(train_chars)\n",
    "print(f\"Charified text:\\n{random_train_chars}\")\n",
    "print(f\"\\nLength of chars: {len(random_train_chars.split())}\")\n",
    "vectorized_chars = char_vectorizer([random_train_chars])\n",
    "print(f\"\\nVectorized chars:\\n{vectorized_chars}\")\n",
    "print(f\"\\nLength of vectorized chars: {len(vectorized_chars[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "48eea234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charified text (before vectorization and embedding):\n",
      "i n   p a s i r e o t i d e   l a r   a n d   o c t r e o t i d e   l a r   p a t i e n t s   ,   r e s p e c t i v e l y   ,   @   %   a n d   @   %   (   p   =   @   )   a c h i e v e d   n o r m a l   i g f - @   ,   a n d   @   %   a n d   @   %   a c h i e v e d   g h   <   @   g / l   .\n",
      "\n",
      "Embedded chars (after vectorization and embedding):\n",
      "[[[-0.00082419 -0.04092801 -0.02829768 ...  0.01858314  0.03517522\n",
      "    0.02097869]\n",
      "  [ 0.02623006  0.01876989 -0.0089603  ...  0.01819288 -0.01415137\n",
      "   -0.01873361]\n",
      "  [-0.04689167  0.01310148 -0.03287587 ... -0.03569833 -0.03927325\n",
      "   -0.02755957]\n",
      "  ...\n",
      "  [ 0.00284646  0.01810196  0.04184692 ...  0.01574563 -0.0180227\n",
      "    0.01962248]\n",
      "  [ 0.00284646  0.01810196  0.04184692 ...  0.01574563 -0.0180227\n",
      "    0.01962248]\n",
      "  [ 0.00284646  0.01810196  0.04184692 ...  0.01574563 -0.0180227\n",
      "    0.01962248]]]\n",
      "\n",
      "Character embedding shape: (1, 290, 25)\n"
     ]
    }
   ],
   "source": [
    "# Create char embedding layer\n",
    "char_embed = layers.Embedding(input_dim=NUM_CHAR_TOKENS, # number of different characters\n",
    "                              output_dim=25, # embedding dimension of each character (same as Figure 1 in https://arxiv.org/pdf/1612.05251.pdf)\n",
    "                              mask_zero=False, # don't use masks (this messes up model_5 if set to True)\n",
    "                              name=\"char_embed\")\n",
    "# Test out character embedding layer\n",
    "print(f\"Charified text (before vectorization and embedding):\\n{random_train_chars}\\n\")\n",
    "char_embed_example = char_embed(char_vectorizer([random_train_chars]))\n",
    "print(f\"Embedded chars (after vectorization and embedding):\\n{char_embed_example}\\n\")\n",
    "print(f\"Character embedding shape: {char_embed_example.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a37ad87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f070c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bbdacd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d866fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6fe2a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ec32b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee12a06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_test_3",
   "language": "python",
   "name": "gpu_test_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
